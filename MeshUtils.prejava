#include "macros.h"

import com.donhatchsw.compat.ArrayList;
import com.donhatchsw.compat.IntArrayList;
import com.donhatchsw.util.Arrays;
import com.donhatchsw.util.FuzzyPointHashTable;
import com.donhatchsw.util.MergeFind;
import com.donhatchsw.util.MyMath;
import com.donhatchsw.util.SortStuff;
import com.donhatchsw.util.VecMath;

final public class MeshUtils
{
    private MeshUtils(){ throw new AssertionError(); } // non-instantiatable util class

    public static double[][/*2*/] getMeshVertsXY(Mesh mesh)
    {
        int nVerts = mesh.verts.size();
        double answer[][] = new double[nVerts][2];

        FORI (iVert, nVerts)
        {
            Mesh.Vertex v = mesh.getVert(iVert);
            answer[iVert][0] = v.x();
            answer[iVert][1] = v.y();
        }
        return answer;
    }

    public static double[][/*3*/] getMeshVertsXYH(Mesh mesh)
    {
        int nVerts = mesh.verts.size();
        double answer[][] = new double[nVerts][3];

        FORI (iVert, nVerts)
        {
            Mesh.Vertex v = mesh.getVert(iVert);
            answer[iVert][0] = v.x();
            answer[iVert][1] = v.y();
            answer[iVert][2] = v.h();
        }
        return answer;
    }

    // If canonicalOrder flag, order each face so that least-index vertex is first,
    // and then sort faces by increasing size and then increasing contents.
    // and then sort faces by increasing size and then contents.
    public static int[][] getMeshFaces(Mesh mesh, boolean canonicalOrderFlag)
    {
        boolean seenEdge[] = new boolean[mesh.edges.size()]; // all false initially
        int[][] faces = new int[mesh.edges.size()][];
        int[] scratchFace = new int[mesh.edges.size()]; // probably safer than mesh.verts.size()
        int nFaces = 0;
        FORIDOWN(iEdge, mesh.edges.size())
        {
            if (seenEdge[iEdge]) continue;
            Mesh.Edge edge0 = mesh.getEdge(iEdge);
            int faceSize = 0;
            Mesh.Edge edge = edge0;
            do {
                seenEdge[edge.myIndex()] = true;
                scratchFace[faceSize++] = edge.initialVertex().myIndex();
            } while ((edge = edge.next()) != edge0);
            faces[nFaces++] = (int[])Arrays.subarray(scratchFace, 0, faceSize);
        }
        faces = (int[][])Arrays.subarray(faces, 0, nFaces);

        if (canonicalOrderFlag)
        {
            FORIDOWN(iFace, faces.length)
            {
                int face[] = faces[iFace];
                int mini = VecMath.mini(face);
                System.arraycopy(face, mini,
                                 scratchFace, 0,
                                 face.length - mini);
                System.arraycopy(face, 0,
                                 scratchFace, face.length - mini,
                                 mini);
                System.arraycopy(scratchFace, 0,
                                 face, 0,
                                 face.length);
            }
            SortStuff.sort(faces, new SortStuff.Comparator() {
                @Override public int compare(Object a, Object b)
                {
                    // TODO: call it aFace, bFace
                    int aFace[] = (int[])a;
                    int bFace[] = (int[])b;
                    if (aFace.length < bFace.length) return -1;
                    if (aFace.length > bFace.length) return 1;
                    for (int i = 0; i < aFace.length; ++i)
                    {
                        if (aFace[i] < bFace[i]) return -1;
                        if (aFace[i] > bFace[i]) return 1;
                    }
                    return 0;
                }
            });
        }
        return faces;
    } // getMeshFaces

    // CBB: assumes mesh is triangulated
    public static int[] findPrimalFaceFromDualVert(Mesh mesh, Mesh dualMesh, int iDualVert)
    {
        int nEdges = dualMesh.edges.size();
        assert_eq(nEdges, mesh.edges.size());
        FORI (iEdge, nEdges)
        {
            Mesh.Edge dualEdge = dualMesh.getEdge(iEdge);
            Mesh.Vertex initialVertex = dualEdge.initialVertex();
            if (initialVertex != null
             && initialVertex.myIndex() == iDualVert)
            {
                Mesh.Edge primalEdge = (mesh.getEdge(iEdge)).opposite();
                assumpt(primalEdge.next().next().next() == primalEdge);
                // advance around triangle til initial vertex is smallest
                while (primalEdge.initialVertex().myIndex() > primalEdge.finalVertex().myIndex()
                    || primalEdge.initialVertex().myIndex() > primalEdge.prev().initialVertex().myIndex())
                    primalEdge = primalEdge.next();
                int i0 = primalEdge.initialVertex().myIndex();
                int i1 = primalEdge.finalVertex().myIndex();
                int i2 = primalEdge.prev().initialVertex().myIndex();
                return new int[] {i0,i1,i2};
            }
        }
        return null;
    } // findPrimalFaceFromDualVert

    private static double detDouble(int n, java.math.BigInteger M[/*n*/][/*n*/])
    {
        int verboseLevel = 1;
        if (verboseLevel >= 1) OUT("            in detDouble(n="+n+")");
        long t0millis = System.currentTimeMillis();
        double Mdouble[][] = new double[n][n];
        for (int i = 0; i < n; ++i)
        for (int j = 0; j < n; ++j)
            Mdouble[i][j] = M[i][j].doubleValue();
        double answer = VecMath.detDestructive(Mdouble);
        long t1millis = System.currentTimeMillis();
        if (verboseLevel >= 1) OUT("            out detDouble(n="+n+"), returning "+answer+" in "+(t1millis-t0millis)/1000.+" secs.");
        return answer;
    }

    // https://en.wikipedia.org/wiki/Bareiss_algorithm
    // http://www.ams.org/journals/mcom/1968-22-103/S0025-5718-1968-0226829-0/S0025-5718-1968-0226829-0.pdf
    // http://cs.nyu.edu/exact/core/download/core_v1.4/core_v1.4/progs/bareiss/bareiss.cpp
    // Crazy good and simple!  I was previously reducing to upper-triangular form using egcd at each step,
    // but it was way more complicated and blew up unless I did strange heuristics.
    // Hmm, it claims to do pivoting, but doesn't, and suffers from zero-divides.
    // there's some analysis here:
    //   http://www.math.usm.edu/perry/Research/Thesis_DRL.pdf
    /*
        double Matrix::determinant() const {
           Matrix A = *this;
           double det;
           int i, j, k;

           for (i = 0; i < n-1; i++) {
              // assert(a(i, i) == 0);
              for (j = i + 1; j < n; j++)
                 for (k = i + 1; k < n; k++) {
                    A(j,k) = (A(j,k)*A(i,i)-A(j,i)*A(i,k));
                    if (i) A(j, k) /= A(i-1,i-1);
                 }
           }
           return A(n-1,n-1);
        }
    */
    private static java.math.BigInteger detDestructive(int n, java.math.BigInteger M[][])
    {
        int verboseLevel = 0;
        if (verboseLevel >= 1) OUT("            in detDestructive(n="+n+")");
        long t0millis = System.currentTimeMillis();
        int maxBitLength = 0;
        java.math.BigInteger answer;
        if (n == 0)
            answer = java.math.BigInteger.ONE;
        else
        {
            int sign = 1;
            FORI (i, n-1)
            {
                if (M[i][i].signum() == 0)
                {
                    // "standard workaround" for zero-divide in bareiss's algorithm, according to thesis.
                    // algorithm is on p.26, this comment is on p.34.
                    // Swap with any row that will make this entry nonzero.
                    int jPivot = -1;
                    for (int j = i+1; j < n; ++j)
                        if (M[j][i].signum() != 0)
                        {
                            jPivot = j;
                            break;
                        }
                    if (jPivot == -1)
                        return java.math.BigInteger.ZERO;
                    OUT("HEY! working around bareiss problem");
                    java.math.BigInteger temp[];
                    SWAP(M[i], M[jPivot], temp);
                    sign *= -1;
                }
                //assert(M[i][i].signum() == 0); XXX what is this?? it fails
                for (int j = i+1; j < n; ++j)
                for (int k = i+1; k < n; ++k)
                {
                    // jk = jk*ii-ji*ik
                    M[j][k] = M[j][k].multiply(M[i][i]).subtract(M[j][i].multiply(M[i][k]));
                    maxBitLength = MAX(maxBitLength, M[j][k].bitLength());
                    if (i > 0)
                    {
                        java.math.BigInteger quotientAndRemainder[/*2*/] = M[j][k].divideAndRemainder(M[i-1][i-1]);
                        M[j][k] = quotientAndRemainder[0];
                        assert(quotientAndRemainder[1].signum() == 0);
                        // that didn't increase the bitlength, so don't need to update maxBitLength
                    }
                }
            }
            answer = M[n-1][n-1];
            if (sign < 0)
                answer = answer.negate();
        }
        long t1millis = System.currentTimeMillis();
        if (verboseLevel >= 1) OUT("              maxBitLength = "+maxBitLength);
        if (verboseLevel >= 1) OUT("            out detDestructive(n="+n+"), returning "+answer+" in "+(t1millis-t0millis)/1000.+" secs.");
        return answer;
    } // detDestructive

    private static java.math.BigInteger[][] copymat(int nRows, int nCols, java.math.BigInteger M[][])
    {
        java.math.BigInteger answer[][] = new java.math.BigInteger[nRows][nCols];
        FORI (iRow, nRows)
        FORI (iCol, nCols)
            answer[iRow][iCol] = M[iRow][iCol];
        return answer;
    }
    private static java.math.BigInteger det(int n, java.math.BigInteger M[][])
    {
        return detDestructive(n, copymat(n, n, M));
    } // det

    private static java.math.BigInteger det(int n, long M[][])
    {
        java.math.BigInteger bigM[][] = new java.math.BigInteger[n][n];
        FORI (i, n)
        FORI (j, n)
            bigM[i][j] = java.math.BigInteger.valueOf(M[i][j]);
        return detDestructive(n, bigM);
    }
    private static double detDouble(int n, long M[][])
    {
        double Mdouble[][] = new double[n][n];
        FORI (i, n)
        FORI (j, n)
            Mdouble[i][j] = (double)M[i][j];
        return VecMath.detDestructive(Mdouble);
    }
    private static java.math.BigInteger det(long M[][])
    {
        return det(M.length, M);
    }
    public static int detConfidenceTest1(long M[][])
    {
        java.math.BigInteger detBig = det(M);
        double detDouble = detDouble(M.length, M);
        boolean same = EQ(detBig.doubleValue(), detDouble, 1e-12);
        OUT("    det("+Arrays.toStringCompact(M)+") = "+detBig+" "+(same?"==":"!=")+" "+detDouble+" "+(same?"yay!":"    OH NO!!!!!!!!!!!!!"));
        int nFailures = same ? 0 : 1;
        return nFailures;
    }
    public static void detConfidenceTest()
    {
        int nFailures = 0;
        // Nonzero determinant
        nFailures += detConfidenceTest1(new long[][]{});
        nFailures += detConfidenceTest1(new long[][]{{1}});
        nFailures += detConfidenceTest1(new long[][]{{2}});
        nFailures += detConfidenceTest1(new long[][]{{-1}});
        nFailures += detConfidenceTest1(new long[][]{{1,0},{0,1}});
        nFailures += detConfidenceTest1(new long[][]{{1,2},{3,4}});
        nFailures += detConfidenceTest1(new long[][]{{0,1},{1,0}});
        nFailures += detConfidenceTest1(new long[][]{{1,0,0},{0,1,0},{0,0,1}});
        nFailures += detConfidenceTest1(new long[][]{{1,0,0},{0,0,1},{0,1,0}});

	// Example from the thesis cited above, where bareiss leads to zero-divide. answer is 245.
        nFailures += detConfidenceTest1(new long[][]{
	    {1,-4,1,2},
	    {-1,4,4,1},
	    {3,3,3,4},
	    {2,5,2,-1},
        });

        // Nonzero determinant, messes up without pivoting
        nFailures += detConfidenceTest1(new long[][]{{0,1,0},{0,0,1},{1,0,0}}); // messes up!
        nFailures += detConfidenceTest1(new long[][]{{0,1,0},{1,0,0},{0,0,1}});
        nFailures += detConfidenceTest1(new long[][]{{0,0,1},{1,0,0},{0,1,0}});
        nFailures += detConfidenceTest1(new long[][]{{0,0,1},{0,1,0},{1,0,0}});

        // Zero determinant
        nFailures += detConfidenceTest1(new long[][]{{0}});
        nFailures += detConfidenceTest1(new long[][]{{0,0},{0,0}});
        nFailures += detConfidenceTest1(new long[][]{{1,0},{0,0}});
        nFailures += detConfidenceTest1(new long[][]{{0,1},{0,0}});
        nFailures += detConfidenceTest1(new long[][]{{0,0},{1,0}});
        nFailures += detConfidenceTest1(new long[][]{{0,0},{0,1}});
        nFailures += detConfidenceTest1(new long[][]{{0,0,0},{0,0,0},{0,0,0}});
        nFailures += detConfidenceTest1(new long[][]{{1,2,3},{4,5,6},{0,0,0}});
        nFailures += detConfidenceTest1(new long[][]{{1,1,1},{2,2,2},{3,3,3}});

        if (true)
        {
            for (int n = 1; n <= 4; ++n)
            {
                FORI (i, 1<<(n*n))
                {
                    long M[][] = new long[n][n];
                    FORI (j, n*n)
                        M[j/n][j%n] = BIT(i, j);
                    nFailures += detConfidenceTest1(M);
                }
            }
        }

        assert_eq(nFailures, 0);
        OUT("detConfidenceTest PASSED! hooray!");
    }

    // Uses Kirchoff's theorem. https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem,
    // with Bareiss's algorithm for exact integer determinant.
    public static java.math.BigInteger countSpanningTrees(Mesh mesh, boolean withAnalysis)
    {
        SimplerMeshDataStructure simplerMeshDataStructure = new SimplerMeshDataStructure(mesh, false);
        int v2e[][] = simplerMeshDataStructure.v2e;
        int e2v[][] = simplerMeshDataStructure.e2v;
        return countSpanningTrees(v2e.length, e2v, withAnalysis);
    } // countSpanningTrees
    public static java.math.BigInteger countSpanningTrees(int nVerts, int e2v[][], boolean withAnalysis)
    {
        if (withAnalysis) OUT("        in countSpanningTrees");
        if (withAnalysis) OUT("          nVerts="+nVerts);
        if (withAnalysis) OUT("          e2v="+Arrays.toStringCompact(e2v));
        if (nVerts == 0) return java.math.BigInteger.ZERO; // special case to avoid determinant of size -1
        java.math.BigInteger laplacianCofactor[][] = new java.math.BigInteger[nVerts-1][nVerts-1];
        for (int i = 0; i < laplacianCofactor.length; ++i)
        for (int j = 0; j < laplacianCofactor[0].length; ++j)
            laplacianCofactor[i][j] = java.math.BigInteger.ZERO;
        int nEdges = e2v.length;
        for (int iEdge = 0; iEdge < nEdges; ++iEdge)
        {
            int i0 = e2v[iEdge][0];
            int i1 = e2v[iEdge][1];
            if (i0 != i1) // self-loops are irrelevant
            {
                if (i0 != nVerts-1) // if in range
                {
                    laplacianCofactor[i0][i0] = laplacianCofactor[i0][i0].add(java.math.BigInteger.ONE);
                    if (i1 != nVerts-1) // if in range
                        laplacianCofactor[i0][i1] = laplacianCofactor[i0][i1].subtract(java.math.BigInteger.ONE);
                }
            }
        }

        if (withAnalysis)
        {
            PRINTMAT(laplacianCofactor);

            double detDouble = detDouble(nVerts-1, laplacianCofactor);
            OUT("          detDouble="+detDouble);
            assert_ge(detDouble, 0);

            java.math.BigInteger detBigInt = det(nVerts-1, laplacianCofactor);
            OUT("          detBigInt="+detBigInt);
            OUT("          detBigInt.doubleValue()="+detBigInt.doubleValue());
            OUT("          detDouble="+detDouble); // again
            assert_ge(detBigInt.signum(), 0);
            assert_almost_eq_rel(detBigInt.doubleValue(), detDouble, 1e-12);

            OUT("        out countSpanningTrees, returning "+detBigInt);
            return detBigInt;
        }
        else
        {
            return detDestructive(nVerts-1, laplacianCofactor);
        }
    } // countSpanningTrees


    // TODO: this paper: https://pdfs.semanticscholar.org/208b/71b285804d96897731d41f9a4079d0523068.pdf
    // says "Colbourn et al. in [9] have proposed an algorithm which runs in O(n^2) time for an n-vertex planar graph!"
    // The Colbourn paper is:
    //  C.J. Colbourn, J.S. Provan, and D. Vertigan: A new approach to solving three combinatorial enumeration problems on planar graphs. Discrete Appl. Math. 60, 119–129 (1995)
    // Cool, found it here: http://www.sciencedirect.com/science/article/pii/0166218X95E01113
    // So let's code it up!
    // Oh argh, does it not use integer arithmetic?  I'm afraid this will blow up immensely.
    // Ah I see, they claim O(n log n) digits accuracy is sufficient.  Hmm.
    //
    // Okay how do I guide the search?
    // Can't seem to find any of the papers that show how to reduce any planar graph
    // in O(n^2) operations, but there is this: https://en.wikipedia.org/wiki/Y-%CE%94_transform
    // which gives some clues...
    // Oh wait!  Check out http://www.wikiwand.com/en/Steinitz's_theorem .
    // It says any convex polyhedron can be transformed to a tetrahedron
    // by delta-Y and Y-deltas?
    public static void messAroundWithDeltaWye(Mesh originalMesh)
    {
        int verboseLevel = 2;
        if (verboseLevel >= 1) OUT("    in messAroundWithDeltaWye");
        if (verboseLevel >= 3) OUT("      originalMesh = "+originalMesh);
        if (false) originalMesh.sanityCheckTopology(); // XXX woops! dual mesh flunks because of the null endpoints!
        Mesh mesh = new Mesh(originalMesh);
        if (verboseLevel >= 3) OUT("      mesh = "+mesh);
        if (false) mesh.sanityCheckTopology(); // XXX woops! dual mesh flunks because of the null endpoints!

        // Hack-- get rid of that nonexistent vertex.
        // (CBB: should something like this be in Mesh?)
        {
            int nEdges = mesh.edges.size();
            boolean hasInfiniteVertex = false;
            FORI (iEdge, nEdges)
                if (mesh.getEdge(iEdge).initialVertex() == null)
                {
                    hasInfiniteVertex = true;
                    break;
                }
            if (hasInfiniteVertex)
            {
                Mesh.Vertex vert = mesh.newVertex(0.,0.,0.,0.); // meaningless
                FORI (iEdge, nEdges)
                {
                    Mesh.Edge edge = mesh.getEdge(iEdge);
                    if (edge.initialVertex() == null)
                        edge.setInitialVertex(vert);
                }
                if (verboseLevel >= 3) OUT("      after fixing: mesh = "+mesh);
            }
            FORI (iEdge, nEdges)
            {
                assert_ne(mesh.getEdge(iEdge).initialVertex(), null);
                assert_ne(mesh.getEdge(iEdge).next(), null);
            }
        }
        mesh.sanityCheckTopology(); // finally it's sane

        while (true)
        {
            if (verboseLevel >= 2) OUT("      top of loop: nVerts="+mesh.verts.size()+" nEdges="+mesh.edges.size());
            // Always go from simpler to more complicated,
            // so that we don't get confused, e.g. a pendant satisfies condition of a series
            // reduction.

            // loop deletion
            {
                if (verboseLevel >= 2) OUT("          checking for loops...");
                Mesh.Edge loop = null;
                {
                    int nEdges = mesh.edges.size();
                    FORI (iEdge, nEdges)
                    {
                        Mesh.Edge edge = mesh.getEdge(iEdge);
                        if (edge.initialVertex() == edge.finalVertex())
                        {
                            loop = edge;
                            break;
                        }
                    }
                }
                if (loop != null)
                {
                    if (verboseLevel >= 2) OUT("              found a loop! deleting it");
                    mesh.deleteEdge(loop);
                    continue;
                }
            }

            // pendant edge deletion
            {
                if (verboseLevel >= 2) OUT("          checking for pendants...");
                Mesh.Edge pendant = null;
                {
                    int nEdges = mesh.edges.size();
                    FORI (iEdge, nEdges)
                    {
                        Mesh.Edge edge = mesh.getEdge(iEdge);
                        Mesh.Edge nextEdge = edge.next();
                        assert_ne(nextEdge, edge); // since no loops
                        assert_eq(nextEdge==edge.opposite(),
                                  edge.finalVertex().arity == 1);
                        if (edge.finalVertex().arity == 1)
                        {
                            pendant = edge;
                            break;
                        }
                    }
                }
                if (pendant != null)
                {
                    if (verboseLevel >= 2) OUT("              found a pendant! deleting it");
                    Mesh.Vertex vert = pendant.finalVertex();
                    // just deleting the vert would do it, but deleting the edge first
                    // is potentially more efficient
                    assert_eq(vert.arity, 1);
                    mesh.deleteEdge(pendant);
                    assert_eq(vert.arity, 0);
                    mesh.deleteVertex(vert, null, null);
                    continue;
                }
            }
            // parallel reduction
            {
                if (verboseLevel >= 2) OUT("          checking for parallels...");
                Mesh.Edge parallel = null;
                {
                    int nEdges = mesh.edges.size();
                    FORI (iEdge, nEdges)
                    {
                        Mesh.Edge edge = mesh.getEdge(iEdge);
                        Mesh.Edge nextEdge = edge.next();
                        assert_ne(nextEdge, edge); // since no loops
                        assert_ne(nextEdge, edge.opposite()); // since no pendants
                        if (nextEdge.finalVertex() == edge.initialVertex())
                        {
                            parallel = edge;
                            break;
                        }
                    }
                }
                if (parallel != null)
                {
                    if (verboseLevel >= 2) OUT("              found a parallel! "+parallel+" deleting it");
                    mesh.deleteEdge(parallel);
                    continue;
                }
            }
            // series reduction
            {
                if (verboseLevel >= 2) OUT("          checking for serieses...");
                Mesh.Edge seriesStart = null;
                {
                    int nEdges = mesh.edges.size();
                    FORI (iEdge, nEdges)
                    {
                        Mesh.Edge edge = mesh.getEdge(iEdge);
                        Mesh.Edge nextEdge = edge.next();
                        assert_ne(nextEdge, edge); // since no loops
                        assert_ne(nextEdge, edge.opposite()); // since no pendants
                        assert_ne(nextEdge.finalVertex(), edge.initialVertex()); // since no parallels
                        assert_eq(edge.next().opposite().next().opposite()==edge,
                                  edge.finalVertex().arity == 2);
                        if (edge.finalVertex().arity == 2)
                        {
                            seriesStart = edge;
                            break;
                        }
                    }
                }
                if (seriesStart != null)
                {
                    if (verboseLevel >= 2) OUT("              found a series! deleting it");

                    if (verboseLevel >= 3) OUT("                  before: "+mesh);

                    // create a replacement edge,
                    // then delete the two originals and the vertex.
                    mesh.sanityCheckTopology();
                    Mesh.Edge shortCut = mesh.newEdge(true);
                    // can't call sanityCheckTopology again until after connected both endpoints
                    // see picture at insertEdgeBefore
                    assert_eq(shortCut.prev(), shortCut.opposite());
                    mesh.insertEdgeBefore(shortCut, seriesStart);
                    assert_ne(shortCut.prev(), shortCut.opposite());
                    assert_eq(shortCut.opposite().prev(), shortCut);
                    mesh.insertEdgeBefore(shortCut.opposite(), seriesStart.next().next());
                    mesh.sanityCheckTopology();
                    Mesh.Vertex vert = seriesStart.finalVertex();
                    assert_eq(vert.arity, 2);
                    mesh.deleteEdge(seriesStart.next());
                    assert_eq(vert.arity, 1);
                    mesh.deleteEdge(seriesStart);
                    assert_eq(vert.arity, 0);
                    mesh.deleteVertex(vert, null, null);
                    mesh.sanityCheckTopology();

                    if (verboseLevel >= 3) OUT("                  after: "+mesh);
                    continue;
                }
            }
            // wye-delta transformation
            {
                if (verboseLevel >= 2) OUT("          checking for wyes...");
                Mesh.Edge towardsWye = null;
                {
                    int nEdges = mesh.edges.size();
                    FORI (iEdge, nEdges)
                    {
                        Mesh.Edge edge = mesh.getEdge(iEdge);
                        assert_eq(edge.next().opposite().next().opposite().next().opposite()==edge,
                                  edge.finalVertex().arity == 3);
                        if (edge.finalVertex().arity == 3)
                        {
                            towardsWye = edge;
                            break;
                        }
                    }
                }
                if (towardsWye != null)
                {
                    if (verboseLevel >= 2) OUT("              found a wye! changing it to a delta");
                    Mesh.Edge A = towardsWye;
                    Mesh.Edge B = A.next().opposite();
                    Mesh.Edge C = B.next().opposite();
                    assert_eq(A, C.next().opposite());
                    Mesh.Edge AB = mesh.newEdge(true);
                    Mesh.Edge BC = mesh.newEdge(true);
                    Mesh.Edge CA = mesh.newEdge(true);
                    mesh.insertEdgeBefore(AB, A);
                    mesh.insertEdgeBefore(BC, B);
                    mesh.insertEdgeBefore(CA, C);
                    mesh.insertEdgeBefore(AB.opposite(), B.opposite().next());
                    mesh.insertEdgeBefore(BC.opposite(), C.opposite().next());
                    mesh.insertEdgeBefore(CA.opposite(), A.opposite().next());

                    mesh.sanityCheckTopology();
                    Mesh.Vertex vert = A.finalVertex();
                    assert_eq(vert.arity, 3);
                    mesh.deleteEdge(A);
                    assert_eq(vert.arity, 2);
                    mesh.deleteEdge(B);
                    assert_eq(vert.arity, 1);
                    mesh.deleteEdge(C);
                    assert_eq(vert.arity, 0);
                    mesh.deleteVertex(vert, null, null);
                    mesh.sanityCheckTopology();

                    continue;
                }
            }
            break; // didn't find any of the constructs
        }
        if (verboseLevel >= 1) OUT("    out messAroundWithDeltaWye");
    } // messAroundWithDeltaWye


    // Simpler mesh data structure, suitable for running
    // the algorithm from David Bruce Wilson
    // "Generating Random Spanning Trees More Quickly than the Cover Time".
    //
    // Subtleties:
    //     - Depending on how original mesh was constructed,
    //       it might have edges leading nowhere or coming from nowhere.
    //       In this case, the "nowhere" gets made into an extra vertex (the "infinite vertex").
    //     - We might or might not want to consider all inside-out vertices
    //       (i.e. those with weight < 0) to be part of the infinite vertex.
    //       This is a param to the constructor.
    //       If true, such vertices get merged into the infinite vertex (which gets created if necessary),
    //       and the original entries of v2e end up empty.

    // CBB: tell me again why I need the param?  doesn't true always work?  I'm confused.
    // Let's get it straight:
    // Calc underside = false (the usual case I work with):
    //      - primal mesh (mostly triangulated) has standard topology, nVerts is unambiguous, all verts rightside out
    //          mergeInsideOutVertsIntoInfiniteVert=false: gives the mesh, unambiguous
    //          mergeInsideOutVertsIntoInfiniteVert=true: gives the mesh, unambiguous
    //      - dual has some edges to nowhere, so converting to standard topology requires adding a vert. all other verts rightside out
    //          mergeInsideOutVertsIntoInfiniteVert=false: gives the mesh with one vertex added
    //          mergeInsideOutVertsIntoInfiniteVert=true: gives the mesh with one vertex added
    // Calc underside = true:
    //      - primal mesh straightforward, as before (except fully triangulated)
    //      - dual mesh has standard topology, but some verts are inside out.
    //          mergeInsideOutVertsIntoInfiniteVert=false: gives the mesh
    //          mergeInsideOutVertsIntoInfiniteVert=true: adds a vertex (since there was at least one inside out), transfers all edges incident
    //              on inside out verts to the added vertex instead, leaving empty entries in v2e.
    //              Note that the resulting structure is not dual to that for the primal mesh!
    // TL;DR: mergeInsideOutVertsIntoInfiniteVert is never relevant for primal,
    // it's only relevant for dual, and then only when "calc underside" is on.
    public static class SimplerMeshDataStructure
    {
        public int v2e[/*nVerts*/][]; // vert to edges out
        public int e2v[/*nEdges*/][/*2*/]; // edge to verts

        // Note, we allow multigraphs and loops
        public static void sanityCheck(int v2e[][], int e2v[][])
        {
            int nVerts = v2e.length;
            int nEdges = e2v.length;
            assert_eq(nEdges % 2, 0);
            FORI (iEdge, nEdges)
            {
                assert_eq(e2v[iEdge][0], e2v[iEdge^1][1]); // other gets checked on the other one
            }
            FORI (iEdge, nEdges)
            {
                int iVert = e2v[iEdge][0];
                int jVert = e2v[iEdge][1];
                assert_le_lt(0, iVert, nVerts);
                assert_le_lt(0, jVert, nVerts);
            }
            int sumOfArities = 0;
            FORI (iVert, nVerts)
            {
                FORI (j, v2e[iVert].length)
                {
                    int iEdge = v2e[iVert][j];
                    assert_le_lt(0, iEdge, nEdges);
                    assert_eq(e2v[iEdge][0], iVert);
                }
                sumOfArities += v2e[iVert].length;
            }
            assert_eq(sumOfArities, nEdges);
        } // sanityCheck

        public SimplerMeshDataStructure(Mesh mesh, boolean mergeInsideOutVertsIntoInfiniteVert)
        {
            int nVerts = mesh.verts.size(); // may add 1 though
            int nEdges = mesh.edges.size();
            this.e2v = new int[nEdges][2];

            int e2next[] = new int[nEdges];
            boolean infiniteVertUsed = false; // until proven otherwise
            FORI (iEdge, nEdges)
            {
                Mesh.Edge edge = mesh.getEdge(iEdge);
                assert_eq(edge.myIndex(), iEdge);
                assert_eq(edge.opposite().myIndex(), (iEdge^1));
                Mesh.Vertex v0 = edge.initialVertex();
                Mesh.Vertex v1 = edge.finalVertex();
                if (v0 == null || v1 == null)
                    infiniteVertUsed = true;
                int i0 = v0==null ? nVerts : v0.myIndex();
                int i1 = v1==null ? nVerts : v1.myIndex();
                e2v[iEdge][0] = i0;
                e2v[iEdge][1] = i1;
                Mesh.Edge next = edge.next();
                if (next == null)
                {
                    // final vertex is the infinite one; to find next, need to walk backwards around the face
                    next = edge;
                    while (next.prev() != null)
                        next = next.prev();
                }
                e2next[iEdge] = next.myIndex();
            }
            if (mergeInsideOutVertsIntoInfiniteVert)
            {
                boolean isInsideOut[] = new boolean[nVerts+1];
                FORI (iVert, nVerts)
                    isInsideOut[iVert] = (mesh.getVert(iVert).weight < 0);
                isInsideOut[nVerts] = true; // the infinite one, if any
                //PRINTVEC(isInsideOut);
                FORI (iEdge, nEdges)
                    FORI (iVertThisEdge, 2)
                    {
                        if (isInsideOut[e2v[iEdge][iVertThisEdge]])
                        {
                            e2v[iEdge][iVertThisEdge] = nVerts;
                            infiniteVertUsed = true;
                        }
                    }
                // Note that it may be that some original verts are now unused in e2v
            }
            if (infiniteVertUsed)
                nVerts++;
            this.v2e = new int[nVerts][];
            {
                int arities[] = new int[nVerts]; // zeros initially
                FORI (iEdge, nEdges)
                    arities[e2v[iEdge][0]]++;
                FORI (iVert, nVerts)
                {
                    v2e[iVert] = new int[arities[iVert]];
                    arities[iVert] = 0;
                }
                if (false)
                {
                    // Simple way, edges don't come out in any particular order.
                    FORI (iEdge, nEdges)
                    {
                        int iVert = e2v[iEdge][0];
                        v2e[iVert][arities[iVert]++] = iEdge;
                    }
                }
                else
                {
                    // Edges come out in order, following the mesh's ->opp->next pointers.
                    // I.e. CCW if next pointers go CW around face,
                    // and vice versa.
                    // CBB: I don't think I really need this bit array; could instead just clear
                    // entries in e2next or something.  But that would be more complicated.
                    boolean edgeDone[] = new boolean[nEdges]; // all false initially
                    FORI(iEdge0, nEdges)
                    {
                        for (int iEdge = iEdge0; !edgeDone[iEdge]; iEdge = e2next[iEdge^1])
                        {
                            edgeDone[iEdge] = true;
                            int iVert = e2v[iEdge][0];
                            v2e[iVert][arities[iVert]++] = iEdge;
                        }
                    }
                }
                // sanity check
                FORI (iVert, nVerts)
                {
                    assert_eq(v2e[iVert].length, arities[iVert]);
                    FORI (iEdgeOut, v2e[iVert].length)
                        assert_eq(e2v[v2e[iVert][iEdgeOut]][0], iVert);
                }
            }
        } // SimplerMeshDataStructure ctor

        // CBB: also have isConnected() above... silly, maybe.
        public boolean isConnectedExceptMaybeForIsolatedVerts()
        {
            // DFS is actually faster supposedly but whatever
            int nVerts = v2e.length;
            int nEdges = e2v.length;
            MergeFind mergeFind = new MergeFind(nVerts);
            FORI (iEdge, nEdges)
            {
                int iVert = e2v[iEdge][0];
                int jVert = e2v[iEdge][1];
                mergeFind.merge(iVert, jVert);
                // CBB: if we use a SizeTrackingMergeFind, can notice as soon as there's a single component, and return true.  but would have to count number of non-isolated verts first
            }
            int found = -1;
            FORI (iVert, nVerts)
            {
                if (v2e[iVert].length == 0) continue; // vertex got removed (or was isolated)
                if (found == -1)
                    found = mergeFind.find(iVert);
                else if (mergeFind.find(iVert) != found)
                    return false;
            }
            return true;
        } // isConnectedExceptMaybeForIsolatedVerts

        // Do not call this unless connected! (in the sense that isConnectedExceptMaybeForIsolatedVerts returns).
        public boolean[/*nEdges/2*/] randomSpanningTree(java.util.Random generator)
        {
            if (e2v.length == 0)
                return new boolean[0];
            // Use uniform random spanning tree algorithm, from David Bruce Wilson
            // "Generating Random Spanning Trees More Quickly than the Cover Time"
            // Must start with random vertex of random edge.
            int r = e2v[generator.nextInt(e2v.length)][generator.nextInt(2)];
            int nVerts = v2e.length;
            boolean inTree[] = new boolean[nVerts]; // false initially
            int theEdgeOut[] = new int[nVerts];
            theEdgeOut[r] = -1;
            inTree[r] = true;
            FORI (iVert, nVerts)
            {
                if (v2e[iVert].length == 0)
                    continue; // the vertex got removed (probably merged into the infinite vertex)
                for (int u = iVert; !inTree[u]; u = e2v[theEdgeOut[u]][1])
                    theEdgeOut[u] = v2e[u][generator.nextInt(v2e[u].length)];
                for (int u = iVert; !inTree[u]; u = e2v[theEdgeOut[u]][1])
                    inTree[u] = true;
            }
            boolean answer[] = new boolean[e2v.length/2]; // all false initially
            FORI (iVert, nVerts)
            {
                if (v2e[iVert].length == 0)
                    continue; // the vertex got removed (probably merged into the infinite vertex)
                int iEdge = theEdgeOut[iVert];
                assert_eq((iEdge == -1), (iVert==r));
                if (iEdge != -1)
                {
                    assert_eq(e2v[iEdge][0], iVert);
                    answer[iEdge/2] = true;
                }
            }
            return answer;
        } // randomSpanningTree

        // Given a spanning tree, expressed in undirectedEdgeIsInSpanningTree
        // (as returned by randomSpanningTree()), and a root,
        // figure out each vertex's "parent" edge, i.e. the edge pointing towards the root.
        public int[] getVertToParentEdgeOut(int root,
                                            boolean undirectedEdgeIsInSpanningTree[/*nUndirectedEdges*/])
        {
            int verboseLevel = 0;
            if (verboseLevel >= 1) System.out.println("in getVertToParentEdgeOut");
            // TODO: BUG: VecMath.toString messes up on int[][], it omits final closing brace
            if (verboseLevel >= 1) System.out.println("  e2v = "+VecMath.toString(e2v));
            if (verboseLevel >= 1) System.out.println("  v2e = "+VecMath.toString(v2e));
            if (verboseLevel >= 1) System.out.println("  undirectedEdgeIsInSpanningTree = "+VecMath.toString(undirectedEdgeIsInSpanningTree));
            int nVerts = this.v2e.length;
            int vertToParentEdgeOut[] = VecMath.fillvec(nVerts, -1);
            int queue[] = new int[nVerts];
            int queueSize = 0;
            queue[queueSize++] = root;
            while (queueSize > 0)
            {
                int iVert = queue[--queueSize];
                int edgesOut[] = this.v2e[iVert];
                FORI (iEdgeOut, edgesOut.length)
                {
                    int iEdge = edgesOut[iEdgeOut];
                    if (iEdge != vertToParentEdgeOut[iVert] // i.e. if didn't just come from there
                     && undirectedEdgeIsInSpanningTree[iEdge/2])
                    {
                        assert_eq(this.e2v[iEdge][0], iVert);
                        int jVert = this.e2v[iEdge][1];
                        int parentEdgeOut = iEdge ^ 1; // opposite edge, i.e. the edge from jVert to iVert
                        vertToParentEdgeOut[jVert] = parentEdgeOut;
                        assert_eq(this.e2v[parentEdgeOut][0], jVert);
                        assert_eq(this.e2v[parentEdgeOut][1], iVert);
                        queue[queueSize++] = jVert;
                    }
                }
            }
            if (verboseLevel >= 1) System.out.println("  vertToParentEdgeOut = "+Arrays.toStringCompact(vertToParentEdgeOut));
            if (verboseLevel >= 1) System.out.println("out getVertToParentEdgeOut");
            return vertToParentEdgeOut;
        } // getVertToParent
    } // SimplerMeshDataStructure

    // See paper "Generating Random Spanning Trees
    // More Quickly than the Cover Time" by David Bruce Wilson.
    // But, can't I do it in O(n) time,
    // by simply growing the tree and picking a random new edge out of it on every step?
    // (answer: no, the next edge out of the spanning tree can't be picked uniformly)
    // Verify that Wilson's algorithm gives something uniformly random,
    // or that the more well known random walk one does,
    // or that a similar one doesn't.
    // Note: This doesn't really do anything useful, it just prints stats, with method
    // chosen via hard-coded true/false's in the code below.
    public static void analyzeRandomSpanningTrees(Mesh mesh)
    {
        System.out.println("    in analyzeRandomSpanningTrees");
        int nEdges = mesh.edges.size();
        PRINT(nEdges);
        if (nEdges > 26) // i.e. 13 undirected.  (takes too long otherwise)
        {
            System.out.println("      too many edges, bailing");
            System.out.println("    out analyzeRandomSpanningTrees");
            return;
        }

        SimplerMeshDataStructure simplerMeshDataStructure = new SimplerMeshDataStructure(mesh, false);
        if (!simplerMeshDataStructure.isConnectedExceptMaybeForIsolatedVerts())
        {
            System.out.println("      graph is disconnected; bailing");
            System.out.println("    out analyzeRandomSpanningTrees");
            return;
        }

        java.util.Random generator = new java.util.Random();

        int counts[] = new int[1<<nEdges];

        int nTries = 1000*1000;
        FORI (iTry, nTries)
        {
            // Make a random spanning tree, and increment its count.

            int whichTree;

            if (true)
            {
                // Wilson's algorithm.
                boolean answer[] = simplerMeshDataStructure.randomSpanningTree(generator);
                whichTree = 0;
                FORI (iUndirectedEdge, answer.length)
                    if (answer[iUndirectedEdge])
                    {
                        assert_lt(iUndirectedEdge, 32);
                        whichTree |= (1 << iUndirectedEdge);
                    }
            }
            else if (false)
            {
                // random walk from 0,
                // keeping *last* edge to each new vertex.
                // This apparently isn't uniform!

                int e2v[][] = simplerMeshDataStructure.e2v;
                int v2e[][] = simplerMeshDataStructure.v2e;
                int nVerts = v2e.length; // may be 1 more than mesh.verts.size(). CBB: error prone!

                boolean inTree[] = new boolean[nVerts]; // false initially
                int prev[] = VecMath.fillvec(nVerts, -1); // vertex to prev *edge*, not vertex
                int r = 0; // arbitrarily

                inTree[r] = true;
                int nVertsInTree = 1;
                int u = r;
                while (nVertsInTree < nVerts)
                {
                    int iEdge = v2e[u][generator.nextInt(v2e[u].length)];
                    u = e2v[iEdge][1];
                    prev[u] = iEdge; // clobber old value if any, so we retain last edge to u
                    if (!inTree[u])
                    {
                        inTree[u] = true;
                        nVertsInTree++;
                    }
                }

                whichTree = 0;
                FORI (iVert, nVerts)
                {
                    int iEdge = prev[iVert];
                    if (iEdge != -1)
                    {
                        whichTree |= (1 << (iEdge/2));
                    }
                }
            }
            else if (false)
            {
                // random walk from 0,
                // keeping *first* edge to each new vertex.
                // This seems to be uniform.
                // (and takes cover time, I think?)

                int e2v[][] = simplerMeshDataStructure.e2v;
                int v2e[][] = simplerMeshDataStructure.v2e;
                int nVerts = v2e.length; // may be 1 more than mesh.verts.size(). CBB: error prone!

                boolean inTree[] = new boolean[nVerts]; // false initially
                int prev[] = VecMath.fillvec(nVerts, -1); // vertex to prev *edge*, not vertex
                int r = 0; // arbitrarily

                inTree[r] = true;
                int nVertsInTree = 1;
                int u = r;
                while (nVertsInTree < nVerts)
                {
                    int iEdge = v2e[u][generator.nextInt(v2e[u].length)];
                    u = e2v[iEdge][1];
                    if (!inTree[u])
                    {
                        prev[u] = iEdge; // only record first edge to u
                        inTree[u] = true;
                        nVertsInTree++;
                    }
                }

                whichTree = 0;
                FORI (iVert, nVerts)
                {
                    int iEdge = prev[iVert];
                    if (iEdge != -1)
                    {
                        whichTree |= (1 << (iEdge/2));
                    }
                }
            }
            else
            {
                assert(false);
            }

            //PRINT(whichTree);

            counts[whichTree]++;
        }
        //PRINTARRAY(counts);

        SortStuff.sort(counts);
        int nZeros = 0;
        while (nZeros < counts.length && counts[nZeros] == 0)
            nZeros++;
        counts = (int[])Arrays.subarray(counts, nZeros, counts.length-nZeros);
        PRINTARRAY(counts);
        PRINT(counts.length);

        System.out.println("    out analyzeRandomSpanningTrees");
    } // analyzeRandomSpanningTrees


    /*
        What about enumerating all spanning trees?
        References:
          http://www.hariharan-ramesh.com/papers/spantree1.pdf
            "Algorithms for Enumerating All Spanning Trees of Undirected and Weighted Graphs"
              By Sanjiv Kapoor, H. Ramesh
            Sounds like it has a particularly nice form of output (1-edge diffs between each tree and next)

          https://en.wikipedia.org/wiki/Kirchhoff's_theorem#Explicit_enumeration_of_spanning_trees
            do the determinant algorithm on intederminates; resulting polynomial's monomials are the possible
            spanning trees

          http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0101-74382005000200004
            "An algorithm to generate all spanning trees of a graph in order of increasing cost"
              by Kenneth Sörensen; Gerrit K. Janssens

          http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-103-77.pdf
            "Finding All Spanning Trees of Undirected and Directed Graphs"
              by Harold N. Gabow

          http://www.cs.ou.edu/~thulasi/Algorithm/Complexity%20of%20Computation%20of%20a%20Spanning%20Tree%20Enumeration%20Algorithm.pdf
            "Complexity of Computation of a Spanning Tree Enumeration Algorithm"
              by R. JAYAKUMAR, K. THULASIRAMAN, AND M. N. S. SWAMY

          http://math.mit.edu/~apost/papers/tree.pdf
            "ENUMERATION OF SPANNING TREES OF GRAPHS"
              by Igor Pak, Alexander Postnikov

    */

#ifdef NOTYET
    /*
        I'm going to try to use Algorithm 2 in Kapoor/Ramesh's paper,
        and I can use the following simplifying assumptions:
          - the graph is biconnected (i.e. no cut vertices), so decomposition into biconnected components is unnecessary
    */
    private static void enumerateAllSpanningTreesSmartWorkInProgress(int v2e[/*nVerts*/][/*2*/], int e2v[/*nEdges*/][])
    {
        // "The data structure will store the current spanning tree in the form of a parent pointer
        // and list of child pointers for each node, with only edges not currently in the IN set present."
        // I think maybe each child list should be a doubly linked list, for O(1) combining.
        // 1. find a dfs spanning tree, for the root node of the computation tree.
        class DFSVert
        {
            public DFSVert(int iVert)
            {
                this.iVert = iVert;
            }
            int iVert;
            DFSVert parent = null;
            DFSVert firstChild = null;
            DFSVert nextSibling = null;
            boolean selfToFirstChildIsBackEdge;
            boolean parentToNextSiblingIsBackEdge;
        }
        int nVerts = v2e.length;
        int nEdges = e2v.length;
        DFSVert dfsVerts[] = new DFSVert[nVerts];
        {
            FORI (iVert, nVerts)
                dfsVerts[iVert] = new DFSVert(iVert);
            FORI (iVert, nVerts)
            {
                FORIDOWN (iChild, v2e[iVert].length)
                {
                    int edge[/*2*/] = e2v[v2e[iVert][iChild]];
                    assert_eq(edge[0], iVert);
                    int jVert = edge[1];
                    dfsVerts[jVert].parent = dfsVerts[iVert];
                    dfsVerts[jVert].nextSibling = dfsVerts[iVert].firstChild;
                    dfsVerts[iVert].firstChild = dfsVerts[iVert];
                }
            }
        }

        static void dfs(DFSVert dfsVert)
        {
            for (DFSVert child = dfsVert.firstChild;
                 child != null;
                 child = child.nextSibling)
            {
                if (child.seen)
                {
                    if (child == dfsVert.firstChild)
                        dfsVert.selfToFirstChildIsBackEdge = true;
                    else
                        previous.parentToNextSiblingIsBackEdge = true;
                }
                else
                {
                }
            }
        }
        boolean v2seen[] = new boolean[nVerts]; // all false initially
        boolean e2isInTree[] = new int[nEdges];
        boolean e2isBack[] = new int[nEdges];

        static void dfs(int v2e[][], int e2v[][],
                        boolean v2seen[], int e2isBack[],
                        int iVert)
        {
            assert(!v2seen[iVert]);
            v2seen[iVert] = true;
            FORI (iChild, v2e[iVert].length)
            {
                int iEdge = v2e[iVert][iChild];
                assert_eq(e2v[iEdge][0], iVert);
                int jVert = e2v[iEdge][1];
                if (v2seen[jVert])
                    e2isBack[iEdge] = true;
                else
                {
                    v2parent[jVert] = iVert;
                    dfs(v2e, e2v, v2seen, e2isBack, jVert);
                }
            }
        }
        FORI (iVert, nVerts)
            if (!v2seen[iVert])
                dfs(v2e, e2v, v2seen, e2isBack, iVert);

        /*
        static void dfs(int iVert, int v2e[][], int e2v[][], DFSVert dfsVerts[])
        {
            DFSVert lastChild = null;
            FORIDOWN (iChild, v2e[iVert].length)
            {
                assert_eq(v2e[iVert][iChild][0], iVert);
                int jVert = v2e[iVert][iChild][1];
                dfsVerts[jVert].nextSibling = dfsVerts[iVert].firstChild;
                dfsVerts[iVert].firstChild = dfsVerts[iVert];
            }
            FORI (iChild, v2e[iVert].length)
            {
                dfsVerts[iVert].
            }
        }
        */

    } // enumerateAllSpanningTrees
#endif // NOTYET

    // http://stackoverflow.com/questions/1235179/simple-way-to-repeat-a-string-in-java/4903603#answer-4903603
    private static String repeat(String s, int n) { return new String(new char[n]).replace("\0", s); }

    // Just to hold stuff for recursion used by enumerateAllSpanningTreesDumb
    public interface enumerateAllSpanningTreesDumbCallback
    {
        boolean run(IntArrayList edgeIndices); // return true iff should continue
    }
        private static class enumerateAllSpanningTreesDumbHelper
        {
            private int nVerts;
            private int e2v[][];
            private int nFaces;
            private int e2f[][];
            enumerateAllSpanningTreesDumbCallback callback;
            MergeFindNewStuff.MergeFindWithUndoInterface vertMergeFind;
            MergeFindNewStuff.MergeFindWithUndoInterface faceMergeFind;
            IntArrayList edgeIndices;
            int verboseLevel;

            public enumerateAllSpanningTreesDumbHelper(
                int nVerts, int e2v[][],
                int nFaces, int e2f[][],
                enumerateAllSpanningTreesDumbCallback callback,
                MergeFindNewStuff.MergeFindWithUndoInterface vertMergeFind,
                MergeFindNewStuff.MergeFindWithUndoInterface faceMergeFind,
                IntArrayList edgeIndices,
                int verboseLevel)
            {
                assert_eq(e2v.length, e2f.length);
                this.nVerts = nVerts;
                this.e2v = e2v;
                this.nFaces = nFaces;
                this.e2f = e2f;
                this.callback = callback;
                this.vertMergeFind = vertMergeFind;
                this.faceMergeFind = faceMergeFind;
                this.edgeIndices = edgeIndices;
                this.verboseLevel = verboseLevel;
            }
            public boolean recurse(int iEdge)
            {
                int nEdges = e2v.length;
                if (verboseLevel >= 2) OUT(repeat("    ",iEdge/2)+"        in recurse(iEdge="+iEdge+"/"+nEdges+")");
                if (true)
                {
                    // This stuff just naturally works out, I guess...
                    // The fact that we don't allow cuts that form cycles
                    // means we can always extend the current cuts to a maximal cut tree,
                    // and the fact that we don't allow folds that form cycles of faces
                    // means we can always extend the current folds to form a maximal fold tree.
                    int nCutsTarget = nVerts - 1;
                    int minPossibleCuts = edgeIndices.size();
                    int maxPossibleCuts = edgeIndices.size() + (nEdges - iEdge);
                    assert_le(minPossibleCuts, nCutsTarget);
                    assert_le(nCutsTarget, maxPossibleCuts);
                }

                if (iEdge == nEdges)
                {
                    assert_eq(edgeIndices.size(), nVerts-1);
                    return callback.run(edgeIndices);
                }
                int iVert = e2v[iEdge][0];
                int jVert = e2v[iEdge][1];
                int iFace = e2f[iEdge][0];
                int jFace = e2f[iEdge][1];
                if (verboseLevel >= 2) OUT(repeat("    ",iEdge/2)+"          iVert="+iVert+" jVert="+jVert+" nVerts="+nVerts+"  iFace="+iFace+" jFace="+jFace+" nFaces="+nFaces);
                if (vertMergeFind.find(iVert) != vertMergeFind.find(jVert))
                {
                    // It's cuttable.
                    vertMergeFind.pushState();
                    vertMergeFind.merge(iVert, jVert);
                    edgeIndices.add(iEdge); // push
                    if (!recurse(iEdge+2)) // we only look at one direction of each edge
                    {
                        if (verboseLevel >= 2) OUT(repeat("    ",iEdge/2)+"        out recurse(iEdge="+iEdge+"/"+nEdges+"), returning false");
                        return false; // note, we didn't pop anything! so unsuitable for further computation
                    }
                    edgeIndices.removeIndex(edgeIndices.size()-1); // pop
                    vertMergeFind.popState();
                }
                if (faceMergeFind.find(iFace) != faceMergeFind.find(jFace))
                {
                    // It's foldable.
                    faceMergeFind.pushState();
                    faceMergeFind.merge(iFace, jFace);
                    if (!recurse(iEdge+2)) // we only look at one direction of each edge
                    {
                        if (verboseLevel >= 2) OUT(repeat("    ",iEdge/2)+"        out recurse(iEdge="+iEdge+"/"+nEdges+"), returning false");
                        return false; // note, we didn't pop anything! so unsuitable for further computation
                    }
                    faceMergeFind.popState();
                }
                if (verboseLevel >= 2) OUT(repeat("    ",iEdge/2)+"        out recurse(iEdge="+iEdge+"/"+nEdges+"), returning true");
                return true;
            }
        } // private static class enumerateAllSpanningTreesDumbHelper

    // Assumption: *not* "Calc underside when delaunayizing".
    // Should assert-fail fairly early due to checking euler's formula, if not.
    public static boolean enumerateAllSpanningTreesDumb(
        Mesh mesh, Mesh dualMesh,  // typically backwards from caller... although it doesn't really matter, this algorithm is self dual
        enumerateAllSpanningTreesDumbCallback callback)
    {
        int verboseLevel = 1;
        if (verboseLevel >= 1) OUT("    in enumerateAllSpanningTreesDumb");
        long t0millis = System.currentTimeMillis();

        int e2v[][] = new SimplerMeshDataStructure(mesh, false).e2v;
        int e2f[][] = new SimplerMeshDataStructure(dualMesh, false).e2v;
        int nEdges = e2v.length;
        assert_eq(nEdges, e2f.length);
        assert_eq(nEdges%2, 0); // directed edges
        int nVerts = mesh.verts.size() + 1;
        int nFaces = dualMesh.verts.size();
        if (verboseLevel >= 1) OUT("      nVerts = "+nVerts);
        if (verboseLevel >= 1) OUT("      nFaces = "+nFaces);
        if (verboseLevel >= 1) OUT("      nEdges = "+nEdges+" = 2*"+(nEdges/2));

        // Check euler's formula, so we don't go down a rabbit hole if they gave us a bad mesh
        if (nVerts+nFaces != nEdges/2 + 2)
        {
            if (verboseLevel >= 1) OUT("    out enumerateAllSpanningTreesDumb, returning finished=true because euler's formula didn't hold (nVerts="+nVerts+" nWholeEdges="+nEdges/2+" nFaces="+nFaces+"");
            return true;
        }

        MergeFindNewStuff.MergeFindWithUndoInterface vertMergeFind = new MergeFindNewStuff.MergeFindWithUndoSmart(nVerts);
        MergeFindNewStuff.MergeFindWithUndoInterface faceMergeFind = new MergeFindNewStuff.MergeFindWithUndoSmart(nFaces);

        IntArrayList edgeIndices = new IntArrayList();
        enumerateAllSpanningTreesDumbHelper helper = new enumerateAllSpanningTreesDumbHelper(
            nVerts, e2v,
            nFaces, e2f,
            callback,
            vertMergeFind,
            faceMergeFind,
            edgeIndices,
            verboseLevel);
        boolean finished = helper.recurse(0);
        long t1millis = System.currentTimeMillis();
        if (verboseLevel >= 1) OUT("    out enumerateAllSpanningTreesDumb, returning finished="+finished+" in "+(t1millis-t0millis)/1000.+" secs");
        return finished;
    } // enumerateAllSpanningTreesDumb

        private static boolean isConnected(int v2e[][], int e2v[][])
        {
            // can be done in O(n), but whatever, this is simpler
            int nVerts = v2e.length;
            int nEdges = e2v.length; // twice number of undirected edges
            boolean doomed = false;
            boolean blessed = false;
            if (nEdges < (nVerts-1)*2)
            {
                //assert(false); // coverage
                return false;
            }
            // note, can't make same kind of shortcut for too many edges,
            // because this might be a multigraph (multiple edges between a given pair of vertices).
            SizeTrackingMergeFind mf = new SizeTrackingMergeFind(v2e.length);
            FORI (iEdge, e2v.length)
            {
                int iVert = e2v[iEdge][0];
                int jVert = e2v[iEdge][1];
                mf.merge(iVert, jVert);
                if (mf.size(iVert) == nVerts)
                {
                    return true;
                }
            }
            return false;
        } // isConnected

        private static int[] selectSpanningTreeRecurse(
            int v2e[][], int e2v[][],
            java.math.BigInteger iNet,
            java.math.BigInteger nNets, // caller should compute as countSpanningTrees(v2e.length, e2v)
            int recursionDepth,
            int verboseLevel)
        {
            if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"        in selectSpanningTreeRecurse(iNet="+iNet+"/"+nNets+", nVerts="+v2e.length+", nEdges="+e2v.length+", recursionDepth="+recursionDepth+")");
            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          v2e = "+Arrays.toStringCompact(v2e));
            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          e2v = "+Arrays.toStringCompact(e2v));
            assert(iNet.signum() >= 0);
            assert(nNets.equals(java.math.BigInteger.valueOf(-1)) || iNet.compareTo(nNets) < 0);

            if (e2v.length == 0)
            {
                assert(v2e.length >= 1);
                // CBB: when blind, sometimes we get rid of all edges but there are still more vertices.
                // so for now, can only assert the following if not blind.  (MAYBE.  let's see how other bugs pans out)
                if (v2e.length == 1)
                    return new int[] {};
                else
                {
                    assert(nNets.signum() < 0); // this only happens when blind
                    return null;
                }
            }

            int iEdge = 0; // or could use last edge, doesn't matter much
            int i0 = e2v[iEdge][0]; // we'll retain this vertex
            int i1 = e2v[iEdge][1]; // we'll delete this vertex

            // TODO: make the contraction and deletion into ops on SimplerMeshDataStructure, or at least functions that return a SimplerMeshDataStructure
            int v2e_edgeContracted[][] = null;
            int e2v_edgeContracted[][] = null;
            if (i0 != i1) // can only be contracted if unequal
            {
                v2e_edgeContracted = (int[][])Arrays.delete(v2e, i1);
                e2v_edgeContracted = (int[][])Arrays.concat(Arrays.subarray(e2v, 0, iEdge),
                                                            Arrays.subarray(e2v, iEdge+2, e2v.length-(iEdge+2)));

                // Make sure we deep copied everything, so that we don't mess with e2v's entries
                v2e_edgeContracted = (int[][])Arrays.copy(v2e_edgeContracted, 2);
                e2v_edgeContracted = (int[][])Arrays.copy(e2v_edgeContracted, 2);

                int i0new = (i0<i1 ? i0 : i0-1);
                // Dump both i0's and i1's edge lists, except for the edge in question (either direction), into i0new's.
                // Don't reindex yet.
                v2e_edgeContracted[i0new] = new int[v2e[i0].length-1 + v2e[i1].length-1];
                {
                    // CBB: get them in the right order
                    int index = 0;
                    FORI (j, v2e[i0].length)
                        if (v2e[i0][j] != iEdge)
                            v2e_edgeContracted[i0new][index++] = v2e[i0][j];
                    FORI (j, v2e[i1].length)
                        if (v2e[i1][j] != (iEdge^1))
                            v2e_edgeContracted[i0new][index++] = v2e[i1][j];
                }
                // Reindex v2e_edgeContracted to account for deletion of iEdge and iEdge^1.
                FORI (i, v2e_edgeContracted.length)
                FORI (j, v2e_edgeContracted[i].length)
                {
                    int iEdgeOld = v2e_edgeContracted[i][j];
                    assert(iEdgeOld != iEdge && iEdgeOld != (iEdge^1));
                    int iEdgeNew = (iEdgeOld > iEdge ? iEdgeOld-2 : iEdgeOld);
                    v2e_edgeContracted[i][j] = iEdgeNew;
                }
                // Reindex e2v_edgeContracted to account for deletion of i1--
                // change references to it to i0, and decrement references to things >i1.
                FORI (i, e2v_edgeContracted.length)
                FORI (j, 2)
                {
                    int iVertOld = e2v_edgeContracted[i][j];
                    int iVertNew = iVertOld==i1 ? i0new : iVertOld>i1 ? iVertOld-1 : iVertOld;
                    e2v_edgeContracted[i][j] = iVertNew;
                }
            }
            int v2e_edgeDeleted[][];
            int e2v_edgeDeleted[][];
            {
                v2e_edgeDeleted = v2e; // for starters
                e2v_edgeDeleted = (int[][])Arrays.concat(Arrays.subarray(e2v, 0, iEdge),
                                                         Arrays.subarray(e2v, iEdge+2, e2v.length-(iEdge+2)));
                // Make sure we deep copied everything, so that we don't mess with e2v's entries
                v2e_edgeDeleted = (int[][])Arrays.copy(v2e_edgeDeleted, 2);
                e2v_edgeDeleted = (int[][])Arrays.copy(e2v_edgeDeleted, 2);

                // Remove iEdge from i0's edge list, and iEdge^1 from i1's edge list.
                v2e_edgeDeleted[i0] = Arrays.delete(v2e_edgeDeleted[i0], Arrays.indexOf(v2e_edgeDeleted[i0], iEdge));
                v2e_edgeDeleted[i1] = Arrays.delete(v2e_edgeDeleted[i1], Arrays.indexOf(v2e_edgeDeleted[i1], iEdge^1));
                // XXX DUP CODE ALERT
                // Reindex v2e_edgeDeleted to account for deletion of iEdge and iEdge^1.
                FORI (i, v2e_edgeDeleted.length)
                FORI (j, v2e_edgeDeleted[i].length)
                {
                    int iEdgeOld = v2e_edgeDeleted[i][j];
                    assert_ne_ne(iEdge, iEdgeOld, iEdge^1);
                    int iEdgeNew = (iEdgeOld > iEdge ? iEdgeOld-2 : iEdgeOld);
                    v2e_edgeDeleted[i][j] = iEdgeNew;
                }
                // No vertices got removed, so no need to reindex e2v_edgeDeleted.
            }

            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          v2e_edgeContracted = "+Arrays.toStringCompact(v2e_edgeContracted));
            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          e2v_edgeContracted = "+Arrays.toStringCompact(e2v_edgeContracted));
            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          v2e_edgeDeleted = "+Arrays.toStringCompact(v2e_edgeDeleted));
            if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          e2v_edgeDeleted = "+Arrays.toStringCompact(e2v_edgeDeleted));
            SimplerMeshDataStructure.sanityCheck(v2e, e2v);
            if (i0 != i1)
                SimplerMeshDataStructure.sanityCheck(v2e_edgeContracted, e2v_edgeContracted);
            SimplerMeshDataStructure.sanityCheck(v2e_edgeDeleted, e2v_edgeDeleted);

            boolean withAnalysis = (verboseLevel >= 3);
            java.math.BigInteger countContracted = java.math.BigInteger.valueOf(-1);
            java.math.BigInteger countDeleted = java.math.BigInteger.valueOf(-1);
            if (nNets.signum() >= 0
             || v2e.length <= 2) // be normal when things get small, just to avoid confusion
            {
                countContracted = i0==i1 ? java.math.BigInteger.ZERO : countSpanningTrees(v2e_edgeContracted.length, e2v_edgeContracted, withAnalysis);
                countDeleted = countSpanningTrees(v2e_edgeDeleted.length, e2v_edgeDeleted, withAnalysis);
                if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          countContracted = "+countContracted);
                if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          countDeleted = "+countDeleted);
                if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          countContracted+countDeleted = "+countContracted.add(countDeleted));
                if (verboseLevel >= 3) OUT(repeat(" ", recursionDepth)+"          nNets = "+nNets);

                if (nNets.signum() >= 0)
                    assert(countContracted.add(countDeleted).equals(nNets));
            }
            else
            {
                // still check whether i0==i1, to prevent going down into insanity
                if (i0==i1)
                    countContracted = java.math.BigInteger.ZERO;
            }

            /*
               How does the decision logic go?
                   if we don't know nNets
                       try going into v2e_edgeContracted.
                       if it worked,
                           return it.
                       else
                           compute countContracted  (CBB: redundant since the call must have known it! I think)
                           use it to go into v2e_edgeDeleted.
                           never really need to compute countDeleted, I don't think, except for sanity checking.
            */

            int answer[] = null;
            if (countContracted.signum() < 0 || iNet.compareTo(countContracted) < 0)
            {
                if (countContracted.signum() >= 0)
                {
                    // contraction guaranteed to succeed, so don't need deleted part any more (CBB: compute it more lazily then! sheesh
                    v2e_edgeDeleted = null;
                    e2v_edgeDeleted = null;
                }
                if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"         trying contracting edge...");
                answer = selectSpanningTreeRecurse(v2e_edgeContracted,
                                                   e2v_edgeContracted,
                                                   iNet,
                                                   countContracted,
                                                   recursionDepth+1,
                                                   verboseLevel);
                if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"         "+(answer!=null?"succeeded.":"failed."));
                assert(nNets.signum()<0 || answer!=null);
                if (answer != null)
                {
                    FORI (i, answer.length) // before we add anything
                        if (answer[i] >= iEdge)
                            answer[i] += 2;
                    answer = (int[])Arrays.prepend(answer, iEdge); // prepend instead of append, to match dumb version
                    if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"        out selectSpanningTreeRecurse(iNet="+iNet+"/"+nNets+", nVerts="+v2e.length+", nEdges="+e2v.length+", recursionDepth="+recursionDepth+"), returning "+Arrays.toStringCompact(answer));
                    // CBB: in this case we didn't need to compute the deleted part!
                    return answer;
                }
                assert(countContracted.signum() < 0);
                countContracted = i0==i1 ? java.math.BigInteger.ZERO : countSpanningTrees(v2e_edgeContracted.length, e2v_edgeContracted, withAnalysis);
            }

            // we computed it either preemptively or lazily
            assert(countContracted.signum() >= 0);
            assert(countContracted.compareTo(iNet) <= 0);

            java.math.BigInteger iNetMinusCountContracted = iNet.subtract(countContracted);
            java.math.BigInteger nNetsMinusCountContracted = nNets.signum()<0?nNets:nNets.subtract(countContracted);


            if (true)
            {
                // See analysis in the caller.
                // Partially defeats the whole blind thing, but does help somewhat.
                if (countDeleted.signum() < 0)
                {
                    countDeleted = countSpanningTrees(v2e_edgeDeleted.length, e2v_edgeDeleted, withAnalysis);
                }
            }

            if (true) // this only makes a difference if the above is false (and didn't save the day, even then)
            {
                if (countDeleted.signum() < 0)
                {
                    // We're not going to compute countDeleted explicitly,
                    // unless it's zero (which is more easily computed than the entire determinant).
                    // I think this is important because it keeps us from doing lots of exploring
                    // down large computation subtrees that don't have any content.
                    if (!isConnected(v2e_edgeDeleted, e2v_edgeDeleted))
                    {
                        countDeleted = java.math.BigInteger.ZERO;
                    }
                }
            }


            if (countDeleted.signum() < 0 || iNetMinusCountContracted.compareTo(countDeleted) < 0)
            {
                v2e_edgeContracted = null;
                e2v_edgeContracted = null;
                if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"         trying deleting edge...");
                answer = selectSpanningTreeRecurse(v2e_edgeDeleted,
                                                   e2v_edgeDeleted,
                                                   iNetMinusCountContracted,
                                                   nNetsMinusCountContracted,
                                                   recursionDepth+1,
                                                   verboseLevel);
                if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"         "+(answer!=null?"succeeded.":"failed."));
                assert(nNets.signum()<0 || answer!=null);
                if (answer != null)
                {
                    FORI (i, answer.length)
                        if (answer[i] >= iEdge)
                            answer[i] += 2;
                }
            }
            if (verboseLevel >= 2) OUT(repeat(" ", recursionDepth)+"        out selectSpanningTreeRecurse(iNet="+iNet+"/"+nNets+", nVerts="+v2e.length+", nEdges="+e2v.length+", recursionDepth="+recursionDepth+"), returning "+Arrays.toStringCompact(answer));
            return answer;
        }
    // returns the selected spanning tree as a list of edge indices, i.e. cuts, all even

    public static int[] selectSpanningTreeOrNull(
        Mesh mesh, Mesh dualMeshUnused,  // typically backwards from caller... although it doesn't really matter, this algorithm is self dual
        java.math.BigInteger iNet)
    {
        int verboseLevel = 2;
        if (verboseLevel >= 1) OUT("    in selectSpanningTreeOrNull(iNet="+iNet+")");
        long t0millis = System.currentTimeMillis();
        if (iNet.signum() < 0)
        {
            OUT("    out selectSpanningTreeOrNull(iNet="+iNet+"), returning null (out of bounds)");
            return null;
        }
        SimplerMeshDataStructure s = new SimplerMeshDataStructure(mesh, false);
        int v2e[][] = s.v2e;
        int e2v[][] = s.e2v;
        boolean withAnalysis = (verboseLevel >= 3);
        // Trying to find a smart way to guide recursion without ever computing full determinant
        // of the whole graph.  No success so far.
        // Experimenting with the following setting:
        //     - knowNNets=true: compute full determinants at all levels.  deterministic, consistent for all iNet, but slow for large nNets
        //                        (so simple recurse becomes much faster as long as iNet not too big)
        //     - knowNNets=false but compute deleted determinant if going down that path (hard coded true or false above, set it to true):
        //              - much faster than knowNNets=true on small
        //              - slower than pure knowNNets=false on small
        //              - consistent (same for small as large)
        //              - so still much faster than knownNNets=true on large
        //              - and much faster than knownNNets=false on large, even better then simple recurse if large enough.  yay!
        //     - knowNNets=false:  pure blind, only compute det of contract part, only *after* it fails.
        //              much faster for small iNet, but super slow for large iNet
        //              (so maybe it's sort of equivalent to simple recurse, but with lots more overhead? bleah)
        //              (seems silly-- if we know contract failed, then we've implicitly
        //              computed the determinant inside it so we don't need to compute it again,
        //              so this can't be right)
        // Oh wait!  try knowNNets=false but turn on computing the deleted determinant when needed!  I think it's fast!
        boolean knowNNets = false;
        java.math.BigInteger nNets = knowNNets ? countSpanningTrees(v2e.length, e2v, withAnalysis)
                                               : java.math.BigInteger.valueOf(-1);
        if (nNets.signum() < 0 && !isConnected(v2e, e2v))
            nNets = java.math.BigInteger.ZERO;
        OUT("      okay, so selecting net "+iNet+"/"+nNets);
        if (nNets.signum() >= 0 && iNet.compareTo(nNets) >= 0)
        {
            OUT("    out selectSpanningTreeOrNull(iNet="+iNet+"/"+nNets+"), returning null (out of bounds)");
            return null;
        }
        int answer[] = selectSpanningTreeRecurse(v2e, e2v, iNet, nNets, 0, verboseLevel);
        long t1millis = System.currentTimeMillis();
        if (verboseLevel >= 1) OUT("    out selectSpanningTreeOrNull(iNet="+iNet+"/"+nNets+"), returning "+Arrays.toStringCompact(answer)+" in "+(t1millis-t0millis)/1000.+" secs.");
        return answer;
    } // selectSpanningTreeOrNull


    public static int[] selectSpanningTreeOrNullDumb(
        Mesh originalMesh, Mesh originalDualMesh, // generally reversed from the calling app's notion.   XXX why am I calling these "original"?
        BigInt iNet)
    {
        if (iNet.lt(0))
            return null;

        final int cutsPtr[][] = {null};
        final BigInt count = new BigInt(0);
        boolean finished = MeshUtils.enumerateAllSpanningTreesDumb(originalMesh, originalDualMesh,
            new MeshUtils.enumerateAllSpanningTreesDumbCallback() {
                public boolean run(IntArrayList cuts)
                {
                    if (count.eq(iNet)) {
                        cutsPtr[0] = cuts.toArray();
                        return false;
                    }
                    count.plusEquals(1);
                    return true;
                }
            });
        PRINT(finished);
        PRINT(count);
        int cuts[] = cutsPtr[0];
        return cuts;
    } // selectSpanningTreeOrNullDumb

    // I seem to have set the bar too high on using VecMath for this... make private utility function instead
    private static double[] v3xm44(double v[/*3*/], double m[/*4*/][/*4*/])
    {
        int three = v.length;
        assert_eq(m.length, three+1);
        assert_eq(m[0].length, three+1);
        double scratch[] = Arrays.append(v, 1.);
        scratch = VecMath.vxm(scratch, m);
        if (scratch[three] == 1.)
            return (double[])Arrays.subarray(scratch, 0, three);
        else
            return VecMath.vxs(three, scratch, scratch[three]);
    }

    public static Mesh.Vertex[] findAllVertsThatAreImages(Mesh mesh, double v[/*2*/], double group[][/*2*/][/*2*/], double tol)
    {
        int nVerts = mesh.verts.size();
        boolean foundVert[] = new boolean[nVerts];
        int nFound = 0;
        FORI (iGroup, group.length)
        {
            assert_eq(v.length, 3);
            double image[] = v3xm44(v,group[iGroup]);
            assert_eq(image.length, 3);
            FORI (iVert, nVerts)
            {
                if (!foundVert[iVert])
                {
                    Mesh.Vertex vertI = mesh.getVert(iVert);
                    // TODO: compute in homo
                    if (EQ(image[0], vertI.x(), tol)
                     && EQ(image[1], vertI.y(), tol)
                     && EQ(image[2], vertI.z(), tol))
                    {
                        foundVert[iVert] = true;
                        nFound++;
                    }
                }
            }
        }
        Mesh.Vertex answer[] = new Mesh.Vertex[nFound];
        nFound = 0;
        FORI (iVert, nVerts)
            if (foundVert[iVert])
                answer[nFound++] = mesh.getVert(iVert);
        assert_eq(nFound, answer.length);
        return answer;
    } // findAllVertsThatAreImages

    // TODO: move these to VecMath I think
    // Normalize so max abs value of coordinates is 1.
    private static void homoNormalize(int n, double answer[], double v[])
    {
        double divideByThis = ABS(v[0]);
        for (int i = 1; i < n; ++i) // skip 0
        {
            double absvi = ABS(v[i]);
            divideByThis = MAX(divideByThis, absvi);
        }
        VecMath.vxs(n, answer, v, 1./divideByThis);
    }
    private static void homoNormalize(double answer[], double v[])
    {
        homoNormalize(MIN(answer.length,v.length), answer, v);
    }
    private static double[] homoNormalize(double v[])
    {
        double answer[] = new double[v.length];
        homoNormalize(v.length, answer, v);
        return answer;
    }

    // Might or might not return the same mesh.
    public static Mesh forceMeshSymmetryByDeletingVertsDestructive(Mesh mesh, double group[][][], boolean retainConnectivity)
    {
        System.out.println("    in forceMeshSymmetryByDeletingVertsDestructive");
        int nVerts = mesh.verts.size();
        System.out.println("          nVerts = "+nVerts);
        double vertsHomo[][] = new double[nVerts][4];
        FORI (iVert, nVerts)
        {
            Mesh.Vertex vert = mesh.getVert(iVert);
            vertsHomo[iVert][0] = vert.X();
            vertsHomo[iVert][1] = vert.Y();
            vertsHomo[iVert][2] = vert.Z();
            vertsHomo[iVert][3] = vert.W();
        }

        // as recommended in FuzzyPointHashTable doc...
        FuzzyPointHashTable fuzzyTable = new FuzzyPointHashTable(1e-12, 1e-10, 1/1024.);
        Object dummy = new Object(); // we really want a hash set, not hash table, so need a dummy object, distinct from null

        // Initial pass adding originals, since those are the preferred images
        FORI (iVert, nVerts)
        {
            if (fuzzyTable.put(homoNormalize(vertsHomo[iVert]),dummy) == null) // i.e. if it wasn't already there
            {
                // nothing. we don't dedup here (although maybe we should).
            }
        }
        // Retain only verts in vertsHomo that are "all there".
        // CBB: probably theres a more efficient algorithm; whatever
        boolean bad[] = new boolean[nVerts];
        int nBad = 0;
        {
            double scratchHomo[] = new double[4];
            double scratchHomoNormalized[] = new double[4];
            FORI (iVert, nVerts)
            {
                for (int iSymmetry = 1; iSymmetry < group.length; ++iSymmetry) // skip 0 since we did it already
                {
                    VecMath.vxm(scratchHomo, vertsHomo[iVert], group[iSymmetry]);
                    homoNormalize(scratchHomoNormalized, scratchHomo);
                    if (fuzzyTable.get(scratchHomoNormalized) == null)
                    {
                        bad[iVert] = true;
                        nBad++;
                        break;
                    }
                }
            }
        }
        if (nBad == 0)
        {
            System.out.println("    out forceMeshSymmetryByDeletingVertsDestructive (nothing changed in "+nVerts+" verts)");
            return mesh;
        }
        if (retainConnectivity)
        {
            // Note, each removal takes O(n) time, so this is quadratic.
            FORIDOWN(iVert, nVerts) // backwards so that deleting won't mess up indices of future work
            {
                if (bad[iVert])
                {
                    mesh.deleteVertex(mesh.getVert(iVert), null, null);
                }
            }
            System.out.println("    out forceMeshSymmetryByDeletingVertsDestructive (deleted "+nBad+"/"+nVerts+" verts in place))");
            return mesh;
        }
        else
        {
            double goodVerts[][] = new double[nVerts-nBad][];
            int nGood = 0;
            FORI (iVert, nVerts)
                if (!bad[iVert])
                    goodVerts[nGood++] = vertsHomo[iVert];
            assert_eq(nGood, goodVerts.length);
            Mesh answer = new Mesh(goodVerts, new int[0][]);
            System.out.println("    out forceMeshSymmetryByDeletingVertsDestructive (created new mesh to delete "+nBad+"/"+nVerts+" verts)");
            return answer;
        }
    } // forceMeshSymmetryByDeletingVertsDestructive

    // Currently destroys connectivity. Another option would be to add verts one by one, the same as if user created them.
    // Currently removes dups.
    // CBB: should vert store notion of whether the intent was H or W?  we assume W.
    // NOTE: I'm using normalized homogeneous coords as keys... however, the reason I'm doing that is because I thought it would fix the behavior on "crack goes in spiral small", but it doesn't!  so maybe go back to using 3d points?  Not sure.  Normalized homo does have the advantage that it should do something reasonable with very large coords though.  Hmm.
    public static Mesh forceMeshSymmetryByReplicatingVerts(Mesh mesh, double group[][][])
    {
        System.out.println("    in forceMeshSymmetryByReplicatingVerts");
        int nVerts = mesh.verts.size();
        System.out.println("          nVerts = "+nVerts);
        double vertsHomo[][] = new double[nVerts][4];
        FORI (iVert, nVerts)
        {
            Mesh.Vertex vert = mesh.getVert(iVert);
            vertsHomo[iVert][0] = vert.X();
            vertsHomo[iVert][1] = vert.Y();
            vertsHomo[iVert][2] = vert.Z();
            vertsHomo[iVert][3] = vert.W();
        }

        // as recommended in FuzzyPointHashTable doc...
        FuzzyPointHashTable fuzzyTable = new FuzzyPointHashTable(1e-12, 1e-10, 1/1024.);
        Object dummy = new Object(); // we really want a hash set, not hash table, so need a dummy object, distinct from null

        ArrayList/*<double[]>*/ finalVertsArrayList = new ArrayList();

        // Initial pass adding originals, since those are the preferred images
        FORI (iVert, nVerts)
        {
            if (fuzzyTable.put(homoNormalize(vertsHomo[iVert]),dummy) == null) // i.e. if it wasn't already there
                finalVertsArrayList.add(vertsHomo[iVert]);
        }
        double scratchHomo[] = new double[4];
        FORI (iVert, nVerts)
        {
            for (int iSymmetry = 1; iSymmetry < group.length; ++iSymmetry) // skip 0 since we did it already
            {
                VecMath.vxm(scratchHomo, vertsHomo[iVert], group[iSymmetry]);
                if (fuzzyTable.put(homoNormalize(scratchHomo),dummy) == null) // i.e. if it wasn't already there
                    finalVertsArrayList.add(VecMath.copyvec(scratchHomo));
            }
        }
        if (finalVertsArrayList.size() == nVerts)
        {
            // Note, connectivity is destroyed otherwise... not sure what's right in general
            System.out.println("    out forceMeshSymmetryByReplicatingVerts (returning original mesh)");
            return mesh;
        }
        double finalVerts[][] = new double[finalVertsArrayList.size()][4];
        finalVertsArrayList.toArray(finalVerts);
        finalVertsArrayList = null;

        Mesh answer = new Mesh(finalVerts, new int[0][]);

        System.out.println("    out forceMeshSymmetryByReplicatingVerts");
        return answer;

    } // forceMeshSymmetryByReplicatingVerts

    public static Mesh changeMeshRotationalSymmetry(Mesh mesh,
                                                    int oldP, int oldQ,
                                                    int newP, int newQ)
    {
        System.out.println("    in changeMeshRotationalSymmetry");
        ArrayList/*<double[]>*/ newVertsArrayList = new ArrayList();

        if (oldQ == 1 && newQ == 1)
        {
            // Old and new fundamental regions are infinite pie slices
            System.out.println("      remap fundamental region pie slice");

            int nOldVerts = mesh.verts.size();
            System.out.println("          nOldVerts = "+nOldVerts);
            double oldVerts[][] = new double[nOldVerts][3];
            FORI (iOldVert, nOldVerts)
            {
                Mesh.Vertex vert = mesh.getVert(iOldVert);
                oldVerts[iOldVert][0] = vert.x();
                oldVerts[iOldVert][1] = vert.y();
                oldVerts[iOldVert][2] = vert.h(); // different from the other (hmm, should this be dependent on wrapAroundSphere instead? now I'm confused)
            }
            double oldBbox[][] = VecMath.bbox(oldVerts);
            double workAreaSize = (nOldVerts==0 ? 1. : MAX4(ABS(oldBbox[0][0]),
                                                            ABS(oldBbox[0][1]),
                                                            ABS(oldBbox[1][0]),
                                                            ABS(oldBbox[1][1])));

            // as recommended in FuzzyPointHashTable doc...
            double littleTol = 1e-12 * workAreaSize;
            double bigTol = 1e-10 * workAreaSize;
            double bucketSize = 1/1024. * workAreaSize;
            FuzzyPointHashTable newVertsTable = new FuzzyPointHashTable(littleTol,
                                                                        bigTol,
                                                                        bucketSize);
            Object dummy = new Object(); // we really want a hash set, not hash table, so need a dummy object, distinct from null

            FORI (iOldVert, nOldVerts)
            {
                double oldVert[] = oldVerts[iOldVert];
                double magnitude = MyMath.hypot(oldVert[0], oldVert[1]);
                double oldAngle = Math.atan2(oldVert[1], oldVert[0]);
                // find fraction in the canonical fundamental region...
                double frac = (oldAngle/(2*Math.PI) + .25) // so -90 degrees produces frac=0
                            * oldP;
                frac -= Math.floor(frac); // so frac should now be between 0 and 1

                if (false) // nah, this looks like hell
                {
                    // make it a conformal mapping (raising to a power in the complex plane)...
                    magnitude = Math.pow(magnitude, (double)oldP/(double)newP);
                }

                FORI (iNewImage, newP)
                {
                    double newAngle = (((iNewImage+frac)) / newP - .25) * (2*Math.PI);
                    double newVert[] = {
                        magnitude * Math.cos(newAngle),
                        magnitude * Math.sin(newAngle),
                    };
                    if (newVertsTable.put(newVert,dummy) == null) // i.e. if it wasn't already there
                    {
                        newVertsArrayList.add(new double[] {
                            newVert[0],
                            newVert[1],
                            oldVert[2], // takes one of the old heights arbitrarily, if multiple.  CBB: can we set it to average?
                        });
                    }
                }
            }
        } else if (oldP != 1 && oldQ != 1
                && newP != 1 && newQ != 1)
        {
            // Old and new fundamental regions are triangles.  Or something.
            System.out.println("      remap fundamental region quad");

            int nOldVerts = mesh.verts.size();
            System.out.println("          nOldVerts = "+nOldVerts);
            double oldVerts[][] = new double[nOldVerts][3];
            FORI (iOldVert, nOldVerts)
            {
                Mesh.Vertex vert = mesh.getVert(iOldVert);
                oldVerts[iOldVert][0] = vert.x();
                oldVerts[iOldVert][1] = vert.y();
                oldVerts[iOldVert][2] = vert.z(); // different from the other  (hmm, should this be dependent on wrapAroundSphere instead? now I'm confused)
            }
            double oldBbox[][] = VecMath.bbox(oldVerts);
            double workAreaSize = (nOldVerts==0 ? 1. : MAX4(ABS(oldBbox[0][0]),
                                                            ABS(oldBbox[0][1]),
                                                            ABS(oldBbox[1][0]),
                                                            ABS(oldBbox[1][1])));


            double oldGroup[][][] = SymmetryUtils.computeSymmetryGroup3d(
                oldP,
                oldQ,
                false, false, // no reflections taken into account here
                false); // repeat work-in-progress not taken into account here, yet

            System.out.println("          "+oldGroup.length+" old symmetries");

            double oldFundamentalRegionVerts[][] = SymmetryUtils.getFundamentalRegionVerts(oldP, oldQ, false);
            int gonality = oldFundamentalRegionVerts.length;
            assert_eq(gonality, 4); // because no reflections
            double oldFundamentalRegionInwardNormals[][] = new double[gonality][3];
            FORI (i, gonality)
            {
                VecMath.vxv3(oldFundamentalRegionInwardNormals[i],
                             oldFundamentalRegionVerts[i],
                             oldFundamentalRegionVerts[(i+1)%gonality]);
                VecMath.normalize(oldFundamentalRegionInwardNormals[i],
                                  oldFundamentalRegionInwardNormals[i]);
            }

            double oldImages[][] = new double[nOldVerts][3];
            {
                double oldImage[] = new double[3]; // scratch for loop
                double fakeBary[] = new double[gonality]; // scratch for loop

                FORI (iOldVert, nOldVerts)
                {
                    double oldVert[] = oldVerts[iOldVert];

                    // Which symmetry brings it into old fundamental region?
                    // It's fuzzy, so make it a contest.
                    double bestGoodness = Double.NEGATIVE_INFINITY;
                    FORI (iOldSymmetry, oldGroup.length)
                    {
                        // CBB: could premultiply the two matrices together, i.e. precompute all images of schwarz triangles. would be less work if lots of oldVerts.
                        VecMath.vxm(oldImage, oldVert, oldGroup[iOldSymmetry]);
                        VecMath.mxv(fakeBary, oldFundamentalRegionInwardNormals, oldImage);
                        double goodness = VecMath.min(fakeBary);
                        if (goodness > bestGoodness)
                        {
                            bestGoodness = goodness;
                            VecMath.copyvec(oldImages[iOldVert], oldImage);
                        }
                    }
                }
            }
            if (true)
            {
                // De-dup oldImages.
                // Note that there can actually still be dups, for images on the boundary of the fundamental region
                // when not reflective symmetry,
                // but comparatively few of them and we'll catch those later.
                // TODO: make a utility function out of deduping?
                int nDeDuped = 0;
                {
                    // TODO: make a FuzzyPointHashSet that does this?

                    Object dummy = new Object(); // we really want a hash set, not hash table, so need a dummy object, distinct from null
                    // as recommended in FuzzyPointHashTable doc...
                    double littleTol = 1e-12 * workAreaSize;
                    double bigTol = 1e-10 * workAreaSize;
                    double bucketSize = 1/1024. * workAreaSize;
                    FuzzyPointHashTable newVertsTable = new FuzzyPointHashTable(littleTol,
                                                                                bigTol,
                                                                                bucketSize);
                    FuzzyPointHashTable fuzzyPointHashTable = new FuzzyPointHashTable(littleTol, bigTol, bucketSize);
                    FORI (iOldVert, nOldVerts)
                        if (newVertsTable.put(oldImages[iOldVert],dummy) == null) // i.e. if it wasn't already there
                            VecMath.copyvec(oldImages[nDeDuped++], oldImages[iOldVert]);
                }
                System.out.println("          old images deduped "+oldImages.length+" -> "+nDeDuped);
                oldImages = (double[][])Arrays.subarray(oldImages, 0, nDeDuped);
            }

            double newImages[][] = new double[oldImages.length][3];

            double newFundamentalRegionVerts[][] = SymmetryUtils.getFundamentalRegionVerts(newP, newQ, false);
            assert_eq(newFundamentalRegionVerts.length, gonality);
            if (gonality == 3)
            {
                assert(false); // it turns out we only do this with rotational symmetry, so this doesn't happen
                double bary[] = new double[3]; // scratch for loop
                FORI (iImage, oldImages.length)
                {
                    double oldMagnitude = VecMath.normalize(oldImages[iImage],oldImages[iImage]); // destructive
                    VecMath.getSphericalAverageWeights(bary, oldFundamentalRegionVerts, oldImages[iImage]);
                    VecMath.sphericalAverage(newImages[iImage], newFundamentalRegionVerts, bary);
                    VecMath.vxs(newImages[iImage], newImages[iImage], oldMagnitude);
                }
            }
            else // gonality == 4
            {
                double oldTris[/*2*/][/*3*/][/*3*/] = {
                    {oldFundamentalRegionVerts[0],oldFundamentalRegionVerts[1],oldFundamentalRegionVerts[2]},
                    {oldFundamentalRegionVerts[0],oldFundamentalRegionVerts[2],oldFundamentalRegionVerts[3]},
                };
                double newTris[/*2*/][/*3*/][/*3*/] = {
                    {newFundamentalRegionVerts[0],newFundamentalRegionVerts[1],newFundamentalRegionVerts[2]},
                    {newFundamentalRegionVerts[0],newFundamentalRegionVerts[2],newFundamentalRegionVerts[3]},
                };
                double bary[] = new double[3]; // scratch for loop
                FORI (iImage, oldImages.length)
                {
                    int which = oldImages[iImage][0]>=0 ? 0 : 1;
                    double oldMagnitude = VecMath.normalize(oldImages[iImage],oldImages[iImage]); // destructive
                    VecMath.getSphericalAverageWeights(bary, oldTris[which], oldImages[iImage]);
                    VecMath.sphericalAverage(newImages[iImage], newTris[which], bary);
                    VecMath.vxs(newImages[iImage], newImages[iImage], oldMagnitude);
                }
            }

            double newGroup[][][] = SymmetryUtils.computeSymmetryGroup3d(
                newP,
                newQ,
                false, false, // no reflections taken into account here
                false); // repeat work-in-progress not taken into account here, yet
            System.out.println("          "+newGroup.length+" new symmetries");
            {
                Object dummy = new Object(); // we really want a hash set, not hash table, so need a dummy object, distinct from null
                // as recommended in FuzzyPointHashTable doc...
                double littleTol = 1e-12 * workAreaSize;
                double bigTol = 1e-10 * workAreaSize;
                double bucketSize = 1/1024. * workAreaSize;
                FuzzyPointHashTable newVertsTable = new FuzzyPointHashTable(littleTol,
                                                                            bigTol,
                                                                            bucketSize);
                double newVert[] = new double[3]; // scratch for loop
                // CBB: probably theres a more efficient algorithm; whatever
                FORI (iImage, newImages.length)
                {
                    FORI (iNewSymmetry, newGroup.length)
                    {
                        VecMath.vxm(newVert, newImages[iImage], newGroup[iNewSymmetry]);
                        if (newVertsTable.put(newVert,dummy) == null) // i.e. if it wasn't already there
                            newVertsArrayList.add(Arrays.append(newVert, 1.)); // xyzw (otherwise would be interpreted as xyh)
                    }
                }
                System.out.println("          new verts deduped "+(newImages.length*newGroup.length)+" -> "+newVertsArrayList.size());
            }

            if (false)
            {
                // HACK-- show the new fundamental region verts
                FORI (i, newFundamentalRegionVerts.length)
                    newVertsArrayList.add(Arrays.append(newFundamentalRegionVerts[i], 1.));
            }
            if (false)
            {
                // HACK-- show old images
                FORI (i, oldImages.length)
                    newVertsArrayList.add(Arrays.append(oldImages[i], 1.));
            }
            if (false)
            {
                // HACK-- show new images
                FORI (i, oldImages.length)
                    newVertsArrayList.add(Arrays.append(newImages[i], 1.));
            }
        }

        double newVerts[][] = new double[newVertsArrayList.size()][];
        newVertsArrayList.toArray(newVerts);
        newVertsArrayList = null;

        // For now, don't try to retain any connectivity.
        // A mesh will appear if "Keep delaunayized" is checked.
        Mesh answer = new Mesh(newVerts, new int[0][]);

        System.out.println("    out changeMeshRotationalSymmetry");
        return answer;
    } // changeMeshRotationalSymmetry

    public static final int FACING_UNKNOWN = 0;
    public static final int FACING_BACK = 1;
    public static final int FACING_SIDE = 2;
    public static final int FACING_FRONT = 3;
    public static void getFacings(double eyeInLocalSpace[],
                                  Mesh mesh, Mesh dualMesh, boolean meshIsReallyDualMesh,
                                  int faceFacings[],
                                  int edgeFacings[],
                                  int vertFacings[],
                                  boolean wrapAroundSphereFlagValue,
                                  boolean centerSphereFlagValue,
                                  double wrapSphereCurvatureValue)
    {
        int verboseLevel = 0;
        if (verboseLevel >= 1) System.out.println("    in getFacings");
        VecMath.fillvec(faceFacings, FACING_UNKNOWN);
        VecMath.fillvec(edgeFacings, FACING_UNKNOWN);
        VecMath.fillvec(vertFacings, FACING_UNKNOWN);

        // XXX wait, is there some confusion here? aren't some of the array sizes one more than this?
        int nVerts = mesh.verts.size();
        int nEdges = mesh.edges.size();
        int nDualVerts = dualMesh.verts.size();

        if (verboseLevel >= 1) System.out.println("      eyeInLocalSpace = "+VecMath.toString(eyeInLocalSpace));
        // compute face facings...
        if (verboseLevel >= 1) System.out.println("      computing face facings");


        // TODO: Do I need to abandon this first method??
        // Problem is, in wrapped-around-sphere case, if whole polyhedron is eyeward of the origin, it's impossible
        // to tell direction from 3d dual vertex position.
        // However, for paraboloid case, it's awesome!  It magically gives the right answer given dual vertex, although I don't understand why.
        if (false)
        {
            FORI (iFace, nDualVerts)
            {
                Mesh.Vertex dualVert = dualMesh.getVert(iFace);

                // plane is all points p such that p dot facePlaneNormal == facePlaneOffset

                double facePlaneNormal[];
                double facePlaneOffset;

                if (wrapAroundSphereFlagValue)
                {
                    // reciprocated wrt sphere.

                    // just work with frickin actual coords,
                    // homogeneous coords are too confusing
                    // and w=0 is exceedingly rare in wrapped-around-sphere case anyway.
                    // this is STILL too frickin complicated!

                    facePlaneNormal = new double[] {
                        dualVert.x(),
                        dualVert.y(),
                        centerSphereFlagValue ? dualVert.z() : dualVert.z() + 1./wrapSphereCurvatureValue
                    };
                    double dualVertDist2FromCenter = VecMath.normsqrd(facePlaneNormal);
                    double facePlaneSmallestPoint[] = VecMath.vxs(facePlaneNormal, 1./dualVertDist2FromCenter);
                    if (!centerSphereFlagValue)
                        facePlaneSmallestPoint[2] -= 1./wrapSphereCurvatureValue;
                    facePlaneOffset = VecMath.dot(facePlaneNormal, facePlaneSmallestPoint);

                    // Hmm, that's wrong if face plane is facing the origin...
                    // and useless if the face plane passes through the origin (in which case the dual vert is at infinity).
                    // How the heck can we tell??
                }
                else
                {
                    // reciprocated wrt paraboloid
                    facePlaneNormal = new double[] {dualVert.X(), dualVert.Y(), dualVert.W()}; // i.e. x,y,1.  not unit length.
                    facePlaneOffset = -dualVert.Z(); // i.e. z.  magic!  I don't know why, but this is the correct value
                }
                // XXX not sure the epsilons make sense... maybe normalize facePlaneNormal and offset, just so we can think straight about that?
                double frontness = VecMath.dot(facePlaneNormal, eyeInLocalSpace)
                          - facePlaneOffset;
                if (LT(frontness, 0., SQR(1e-6)))
                    faceFacings[iFace] = FACING_BACK;
                else if (GT(frontness, 0., SQR(1e-6)))
                    faceFacings[iFace] = FACING_FRONT;
                else
                    faceFacings[iFace] = FACING_SIDE;
            }
        }
        else
        {
            if (verboseLevel >= 1) System.out.println("          (NOT!)");
        }

        // Bleah, the faces that don't have dual verts
        // need to be computed by a separate method.
        // (and the above method sucks anyway, at least for wrapped-around-sphere).
        if (verboseLevel >= 1) System.out.println("      computing face for faces that don't have dual verts");
        FORI (iEdge, nEdges)
        {
            Mesh.Edge edgeI = mesh.getEdge(iEdge);
            Mesh.Edge dualEdgeI = dualMesh.getEdge(iEdge);
            Mesh.Vertex leftDualVert = dualEdgeI.finalVertex();
            int iLeftFace = leftDualVert==null ? nDualVerts : leftDualVert.myIndex();
            if (faceFacings[iLeftFace] == FACING_UNKNOWN)
            {
                if (verboseLevel >= 2) System.out.println("          iEdge = e"+iEdge+"/"+nEdges+" "+(edgeI.initialVertex()==null?"null":"v"+edgeI.initialVertex().myIndex())+"->"+(edgeI.finalVertex()==null?"null  ":"v"+edgeI.finalVertex().myIndex())+" ("+(edgeI.direction!=null?"has direction":"no direction")+")");
                double normal[] = {0,0,0}; // for starters
                double moment[] = {0,0,0}; // for starters
                double centroid[] = null;
                // XXX TODO: centroid of verts might be more robust than area centroid? yeah I think so, hmm
                double area = 0.;
                {
                    Mesh.Vertex v0 = edgeI.initialVertex();
                    if (v0 == null)
                    {
                        if (verboseLevel >= 2) System.out.println("              ouch! edge "+iEdge+" has null initial vertex ("+(edgeI.direction!=null?"has direction":"no direction")+")");
                        // This one shouldn't be a disaster; this face will be solved by some other edge on it.
                        continue;
                    }
                    // CBB: compute in homogeneous space?  Hmm that would be ambitious.
                    assert_ne(v0, null);
                    double v0coords[] = {v0.x(), v0.y(), v0.z()};
                    double v1coords[] = new double[3]; // scratch for loop
                    double v2coords[] = new double[3]; // scratch for loop
                    double thisCentroid[] = new double[3]; // scratch for loop
                    double thisWeightedNormal[] = new double[3]; // scratch for loop
                    Mesh.Edge edgeJ;
                    for (edgeJ = edgeI.next(); edgeJ.next() != edgeI; edgeJ = edgeJ.next())
                    {
                        Mesh.Vertex v1 = edgeJ.initialVertex();
                        Mesh.Vertex v2 = edgeJ.finalVertex();
                        // a bit hackish, but prevents weird computation using vertices we shouldn't be using
                        if (v1 != null && v1.weight < 0.) v1 = null;
                        if (v2 != null && v2.weight < 0.) v2 = null;
                        if (v1 == null && v2 == null)
                        {
                            // Happens when not "calc underside when delaunayizing"
                            // Not quite sure yet how to prevent these things from being drawn.  (e.g. the second sheet, when convex noise 1 starts from a square and I don't re-delaunayize with "calc underside when delaunayizing"=false at end)
                            if (verboseLevel >= 0) System.out.println("                  ouch! edge "+edgeJ.myIndex()+" has null initial and final vertex ("+(edgeJ.direction!=null?"has direction":"no direction")+")");
                            break;
                        }
                        else if (v1 == null)
                        {
                            if (verboseLevel >= 2) System.out.println("                  ouch! edge "+edgeJ.myIndex()+" has null initial vertex ("+(edgeJ.direction!=null?"has direction":"no direction")+")");
                            assert_ne(edgeJ.direction, null);
                            v2coords[0] = v2.x();
                            v2coords[1] = v2.y();
                            v2coords[2] = v2.z();
                            VecMath.vmv(3, v1coords, v2coords, edgeJ.direction);
                        }
                        else if (v2 == null)
                        {
                            if (verboseLevel >= 2) System.out.println("                  ouch! edge "+edgeJ.myIndex()+" has null final vertex ("+(edgeJ.direction!=null?"has direction":"no direction")+")");
                            assert_ne(edgeJ.direction, null);
                            v1coords[0] = v1.x();
                            v1coords[1] = v1.y();
                            v1coords[2] = v1.z();
                            VecMath.vpv(3, v2coords, v1coords, edgeJ.direction);
                        }
                        else
                        {
                            v1coords[0] = v1.x();
                            v1coords[1] = v1.y();
                            v1coords[2] = v1.z();
                            v2coords[0] = v2.x();
                            v2coords[1] = v2.y();
                            v2coords[2] = v2.z();
                        }
                        VecMath.vmv(v1coords, v1coords, v0coords); // so now relative to v0
                        VecMath.vmv(v2coords, v2coords, v0coords); // so now relative to v0
                        VecMath.vxv3(thisWeightedNormal, v1coords, v2coords);
                        VecMath.vpv(normal, normal, thisWeightedNormal);
                        if (verboseLevel >= 2) System.out.println("                  jEdge = e"+edgeJ.myIndex()+"/"+nEdges+" "+(edgeJ.initialVertex()==null?"null":"v"+edgeJ.initialVertex().myIndex())+"->"+(edgeJ.finalVertex()==null?"null  ":"v"+edgeJ.finalVertex().myIndex())+" adding weighted normal "+VecMath.toString(thisWeightedNormal));
                        double thisArea = VecMath.norm(thisWeightedNormal);
                        area += thisArea;
                        VecMath.sxvpsxv(thisCentroid, 1/3., v1coords, 1/3., v2coords); // relative to v0
                        VecMath.vpsxv(moment, moment, thisArea, thisCentroid);
                    }
                    if (edgeJ != edgeI) // if broke
                    {
                        if (verboseLevel >= 2) System.out.println("              ouch! continuing");
                    }
                    centroid = VecMath.vpsxv(v0coords, 1./area, moment);
                }
                if (verboseLevel >= 2) System.out.println("              normal = "+VecMath.toString(normal));
                if (verboseLevel >= 2) System.out.println("              eyeInLocalSpace-centroid = "+VecMath.toString(VecMath.vmv(eyeInLocalSpace, centroid)));
                double frontness = VecMath.dot(normal, eyeInLocalSpace)
                                 - VecMath.dot(normal, centroid);
                if (meshIsReallyDualMesh)
                    frontness *= -1;
                if (verboseLevel >= 2) System.out.println("              frontness = "+frontness);
                // XXX not sure the epsilons make sense... maybe normalize normal, just so we can think straight about that?
                if (LT(frontness, 0., SQR(1e-6)))
                    faceFacings[iLeftFace] = FACING_BACK;
                else if (GT(frontness, 0., SQR(1e-6)))
                    faceFacings[iLeftFace] = FACING_FRONT;
                else
                    faceFacings[iLeftFace] = FACING_SIDE;
                //PRINTVEC(normal);
            }
        }

        // TODO: explain exactly what UNKNOWN in faceFacings means
        if (verboseLevel >= 2)
        {
            FORI (i, faceFacings.length)
                if (faceFacings[i] == FACING_UNKNOWN)
                    OUT("HEY! faceFacings["+i+"/"+faceFacings.length+"] is UNKNOWN!");
        }

        // compute edge facings from incident face facings...
        if (verboseLevel >= 1) System.out.println("      computing edge facings");
        FORI (iEdge, nEdges)
        {
            if (edgeFacings[iEdge] == FACING_UNKNOWN)
            {
                Mesh.Edge edgeI = mesh.getEdge(iEdge);
                Mesh.Edge dualEdgeI = dualMesh.getEdge(iEdge);
                Mesh.Vertex leftDualVert = dualEdgeI.finalVertex();
                Mesh.Vertex rightDualVert = dualEdgeI.initialVertex();
                int iLeftFace = leftDualVert==null ? nDualVerts : leftDualVert.myIndex();
                int iRightFace = rightDualVert==null ? nDualVerts : rightDualVert.myIndex();
                int leftFaceFacing = faceFacings[iLeftFace];
                int rightFaceFacing = faceFacings[iRightFace];
                // UNKNOWN trumps all others
                edgeFacings[iEdge] = (leftFaceFacing==FACING_UNKNOWN || rightFaceFacing==FACING_UNKNOWN) ? FACING_UNKNOWN
                                   : leftFaceFacing==rightFaceFacing ? leftFaceFacing
                                   : FACING_SIDE;
                int oEdge = edgeI.opposite().myIndex();
                assert_eq(oEdge, (iEdge^1));
                edgeFacings[oEdge] = edgeFacings[iEdge];
            }
        }

        // TODO: explain exactly what UNKNOWN in edgeFacings means
        if (verboseLevel >= 2)
        {
            FORI (i, edgeFacings.length)
                if (edgeFacings[i] == FACING_UNKNOWN)
                    OUT("HEY! edgeFacings["+i+"/"+edgeFacings.length+"] is UNKNOWN!");
        }

        // compute vert facings from incident edge facings...
        if (verboseLevel >= 1) System.out.println("      computing vert facings");
        // bleah, need some other value for "not yet initialized"
        assert_ne(-1, FACING_UNKNOWN);
        VecMath.fillvec(vertFacings, -1);
        FORI (iEdge, nEdges)
        {
            // only need to do initial vertex-- edge.opposite() will do final vertex
            Mesh.Vertex v0 = mesh.getEdge(iEdge).initialVertex();
            if (v0 == null)
                continue;
            int i0 = v0.myIndex();
            if (vertFacings[i0] == -1) // uninitialized
                vertFacings[i0] = edgeFacings[iEdge];
            else if (vertFacings[i0] == FACING_UNKNOWN || edgeFacings[iEdge] == FACING_UNKNOWN)
                vertFacings[i0] = FACING_UNKNOWN;
            else if (vertFacings[i0] != edgeFacings[iEdge])
                vertFacings[i0] = FACING_SIDE;
            // otherwise leave it what it is
        }
        FORI (iVert, vertFacings.length)
        {
            if (vertFacings[iVert] == -1)
                vertFacings[iVert] = FACING_UNKNOWN;
        }

        // UNKNOWN in vertFacings means either it's an isolated vert,
        // or it's incident on an edge with UNKNOWN facing.

        if (verboseLevel >= 1)
        {
            OUT("      --------");
            final char facingToChar[] = {'?','-','0','+'};
            {
                StringBuffer sb = new StringBuffer();
                FORI (i, faceFacings.length)
                    sb.append(facingToChar[faceFacings[i]]);
                System.out.println("      face facings: "+sb);
            }
            {
                StringBuffer sb = new StringBuffer();
                FORI (i, edgeFacings.length)
                    sb.append(facingToChar[edgeFacings[i]]);
                System.out.println("      edge facings: "+sb);
            }
            {
                StringBuffer sb = new StringBuffer();
                FORI (i, vertFacings.length)
                    sb.append(facingToChar[vertFacings[i]]);
                System.out.println("      vert facings: "+sb);
            }
            OUT("      --------");
        }
        if (verboseLevel >= 1) System.out.println("    out getFacings");
    } // getFacings

    // CBB: probably doesn't work unless faces are convex
    // CBB: point can fall through cracks
    public static Mesh.Edge findSomeEdgeOnFaceVertIsIn(Mesh mesh, double x, double y)
    {

        boolean kissed = false;
        double twiceMostNegativeFaceArea = Double.POSITIVE_INFINITY;
        Mesh.Edge edgeOnMostNegativeFace = null;
        // XXX traverses each face of size n n times!  need to keep track of what we've done already
        int nEdges = mesh.edges.size();
        FORI (iEdge, nEdges)
        {
            boolean thisFaceIsGoodSoFar = true;
            Mesh.Edge edgeI = mesh.getEdge(iEdge);
            double twiceThisFaceArea = 0.;
            for (Mesh.Edge edge = edgeI;;)
            {
                double twiceThisTriArea = GeomUtils.twiceTriangleArea(
                    x, y,
                    edge.initialVertex().x(),
                    edge.initialVertex().y(),
                    edge.finalVertex().x(),
                    edge.finalVertex().y());
                if (twiceThisTriArea < 0)
                {
                    thisFaceIsGoodSoFar = false;
                    break; // out of this face
                }

                if ((edge = edge.next()) == edgeI)
                    break;
            }
            if (thisFaceIsGoodSoFar)
                return edgeI;
        }
        return null;
    } // findSomeEdgeOnFaceVertIsIn

    public static Mesh.Edge findSomeEdgeOnMostNegativeFace(Mesh mesh)
    {
        double twiceMostNegativeFaceArea = Double.POSITIVE_INFINITY;
        Mesh.Edge answer = null;
        int nEdges = mesh.edges.size();
        FORI (iEdge, nEdges)
        {
            Mesh.Edge edgeI = mesh.getEdge(iEdge);
            double x0 = edgeI.initialVertex().x();
            double y0 = edgeI.initialVertex().y();
            double twiceThisFaceArea = 0.;
            for (Mesh.Edge edge = edgeI;;)
            {
                double twiceThisTriArea = GeomUtils.twiceTriangleArea(
                    x0, y0,
                    edge.initialVertex().x(),
                    edge.initialVertex().y(),
                    edge.finalVertex().x(),
                    edge.finalVertex().y());
                twiceThisFaceArea += twiceThisTriArea;

                if ((edge = edge.next()) == edgeI)
                    break;
            }
            if (twiceThisFaceArea < twiceMostNegativeFaceArea)
            {
                twiceMostNegativeFaceArea = twiceThisFaceArea;
                answer = edgeI;
            }
        }
        return answer;
    } // findSomeEdgeOnMostNegativeFace

    public static double[][/*3*/] parseRadialHeightFieldPairs(String text)
    {
        String tokens[] = com.donhatchsw.compat.regex.split(text.trim(), "\\s+");
        if (tokens.length == 1 && tokens[0].equals("")) tokens = new String[]{}; // fix tokenization
        //PRINTARRAY(tokens);
        if (tokens.length % 2 != 0) return null;
        double pairs[][] = new double[tokens.length/2][2];
        FORI (i, tokens.length)
        {
            try
            {
                pairs[i/2][i%2] = Double.parseDouble(tokens[i]);
            }
            catch (NumberFormatException err)
            {
                return null;
            }
        }
        for (int i = 1; i < pairs.length; ++i)
        {
            if (!(pairs[i][0] >= (i==0?0.:pairs[i-1][0]))) return null; // radii must be nonnegative and nondecreasing
            if (!(pairs[i][1] >= (i==0?0.:pairs[i-1][1]))) return null; // dual radii must be nonnegative and nondecreasing
        }
        return pairs;
    } // parseRadialHeightFieldPairs

    // Sample:
    //   .06 .095 .14 .105    .16 .195 .24 .205    .26 .295  .34 .305  .36 .395  .44 .405  .46 .495
    //
    // Did this:
    //   Just noise 1000
    //   delaunayized on
    //   prewarp
    //   apply radial match (with above)
    //   delaunayized off
    //   "maybe make more difficult by truncating"
    //   start with upward Net
    //   make good
    // Haha, thought I found a counterexample for a few minutes!  But I think I applied radial match with delaunayized off :-(
    //
    public static void preWarpMeshForRadialHeightField(Mesh mesh, double pairs[][/*2*/])
    {
        if (pairs.length > 0) // prevent index out of bounds
        {
            FORIDOWN (iVert, mesh.verts.size())
            {
                Mesh.Vertex vert = mesh.getVert(iVert);
                // Treat initial x,y as target x,y in dual
                double xInDual = vert.x();
                double yInDual = vert.y();
                double rInDual = MyMath.hypot(xInDual, yInDual);

                // Find rInPrimal by mapping backwards.
                // Find interval [r0,r1) containing rInDual...
                // i is index of the upper end.
                int i = 0;
                while (i < pairs.length && rInDual >= pairs[i][1])
                    i++;
                double r0InDual = (i==0 ? 0. : pairs[i-1][1]);
                double r1InDual = (i==pairs.length ? pairs[i-1][1]+1+rInDual : pairs[i][1]);
                assert_le_lt(r0InDual, rInDual, r1InDual);
                double frac = (rInDual-r0InDual) / (r1InDual-r0InDual);
                assert_le_lt(0, frac, 1);
                double r0InPrimal = (i==0 ? 0. : pairs[i-1][0]);
                double r1InPrimal = (i==pairs.length ? pairs[i-1][0]+1+rInDual : pairs[i][0]);
                double rInPrimal = LERP(r0InPrimal, r1InPrimal, frac);
                double scale = rInPrimal==0. ? 42. : rInPrimal / rInDual; // avoid zero divide
                double xInPrimal = xInDual * scale;
                double yInPrimal = yInDual * scale;
                if (false)
                {
                    OUT("===============");
                    PRINT(xInDual);
                    PRINT(yInDual);
                    PRINT(rInDual);
                    PRINT(r0InDual);
                    PRINT(r1InDual);
                    PRINT(frac);
                    PRINT(r0InPrimal);
                    PRINT(r1InPrimal);
                    PRINT(xInPrimal);
                    PRINT(yInPrimal);
                    OUT("===============");
                }
                vert.setxyh(xInPrimal, yInPrimal, vert.h());
            }
        }
    } // preWarpMeshForRadialHeightField
    public static void unPreWarpMeshForRadialHeightField(Mesh mesh, double pairs[][/*2*/])
    {
        double reversedPairs[][] = new double[pairs.length][2];
        FORI (i, pairs.length)
        {
            reversedPairs[i][0] = pairs[i][1];
            reversedPairs[i][1] = pairs[i][0];
        }
        preWarpMeshForRadialHeightField(mesh, reversedPairs);
    } // unPreWarpMeshForRadialHeightField


    // TODO: move to MeshUtils probably
    // Apply a radial height field.
    // The spec is an array of pairs {primalRadius, dualRadius},
    // (where each dualRadius manifests as a slope in the primal).

    // Hmm, trying to solve the following from outward net only makes it worse:
    // (this was when I expressed the radial height field as triples)
    //      blue noise 4000, delaunayized
    //      0 0 .1  .1 0 10  .2 0 .1 .3 0 10  .4 0 .1  .5 0 10
    //
    //      just noise 1000, delaunayized
    //      0 0 .1  .2 .25 .1  .4 1 .1 
    // however, starting from upwards net usually solves it.  hmm.
    public static void applyRadialHeightFieldPairsToMesh(Mesh mesh, double pairs[][])
    {
        int verboseLevel = 2; // 1: in/out, 2: nice description, 3: gory
        if (verboseLevel >= 1) OUT("    in applyRadialHeightFieldPairsToMesh(pairs="+Arrays.toStringCompact(pairs)+")");
        // Assumes all values are nonnegative and nondecreasing, since generated by parseRadialHeightFieldPairs

        double radii[] = new double[pairs.length + 1];
        double positions[] = new double[pairs.length + 1];
        double velocities[] = new double[pairs.length + 1];
        double accelerations[] = new double[pairs.length + 1];
        {
            double r = 0.; // accumulated radius in primal
            double p = 0.; // accumulated position (height)
            double v = 0.; // accumulated velocity (radius in dual)
            double a = 0.; // accumulated acceleration
        }
        if (true)
        {
            radii[0] = 0.;
            positions[0] = 0.;
            velocities[0] = 0.;
            FORI (i, pairs.length)
            {
                radii[i+1] = pairs[i][0];
                velocities[i+1] = pairs[i][1];
                double dt = radii[i+1] - radii[i];
                double dv = velocities[i+1] - velocities[i];
                accelerations[i] = dv / dt; // ok if infinite, that means this interval won't be used for calculations
                if (false)
                {
                    // Correct but blows up if acceleration is infinite
                    positions[i+1] = positions[i] + velocities[i]*dt + .5*accelerations[i]*(dt*dt);
                }
                else
                {
                    // Note that accelerations[i]*(dt*dt) == dv*dt,
                    // which avoids zero-divide.
                    positions[i+1] = positions[i] + velocities[i]*dt + .5*dv*dt;
                }
            }
            accelerations[pairs.length] = 1.; // so primal matches dual out to infinity
        }

        if (true)
        {
            if (verboseLevel >= 2) OUT("      Description:");
            FORI (i, pairs.length+1)
            {
                if (verboseLevel >= 2) OUT("          r = "+radii[i]+" (radius in primal)");
                if (verboseLevel >= 2) OUT("              p = "+positions[i]);
                if (verboseLevel >= 2) OUT("              v = "+velocities[i]+" (radius in dual)");
                if (verboseLevel >= 2) OUT("              a = "+accelerations[i]);
            }
        }
        if (verboseLevel >= 2) PRINTVEC(radii);
        if (verboseLevel >= 2) PRINTVEC(positions);
        if (verboseLevel >= 2) PRINTVEC(velocities);
        if (verboseLevel >= 2) PRINTVEC(accelerations);

        FORIDOWN (iVert, mesh.verts.size())
        {
            Mesh.Vertex vert = mesh.getVert(iVert);
            double x = vert.x();
            double y = vert.y();
            double r = MyMath.hypot(x, y);
            if (verboseLevel >= 3) OUT("==========");
            if (verboseLevel >= 3) PRINT(iVert);
            if (verboseLevel >= 3) PRINT(x);
            if (verboseLevel >= 3) PRINT(y);
            if (verboseLevel >= 3) PRINT(r);

            int i = 0;
            // could do binary search but whatever.
            // find the interval [r0,r1) that contains r.
            while (i+1 < radii.length && r >= radii[i+1])
                i++;
            if (verboseLevel >= 3) PRINT(i);

            double t = r - radii[i];
            if (verboseLevel >= 3) PRINT(t);
            assert_ne(1./accelerations[i], 0.); // not infinite
            assert_eq(accelerations[i], accelerations[i]); // not NaN
            double h = positions[i] + velocities[i]*t + .5*accelerations[i]*(t*t);
            if (verboseLevel >= 3) PRINT(h);

            // we were doing positive, but really should be concave with mouth downwards
            h *= -1;
            if (verboseLevel >= 3) PRINT(h);
            // change to offset from canonical paraboloid
            h -= -.5*r*r;
            if (verboseLevel >= 3) PRINT(h);
            if (verboseLevel >= 3) OUT("==========");

            vert.setxyh(x, y, h);
        }
        if (verboseLevel >= 1) OUT("    out applyRadialHeightFieldPairsToMesh(pairs="+Arrays.toStringCompact(pairs)+")");
    } // applyRadialHeightFieldPairsToMesh

    public static Mesh.Edge[] getVertToFirstEdgeOut(Mesh mesh)
    {
        int nVerts = mesh.verts.size();
        int nEdges = mesh.edges.size();
        Mesh.Edge answer[] = new Mesh.Edge[nVerts]; // all null initially
        FORI (iEdge, nEdges)
        {
            Mesh.Edge edge = mesh.getEdge(iEdge);
            Mesh.Vertex initialVertex = edge.initialVertex();
            if (initialVertex != null)
            {
                int iVert = initialVertex.myIndex();
                if (answer[iVert] == null)
                    answer[iVert] = edge;
            }
        }
        return answer;
    } // getVertToFirstEdgeOut

    // Greedily find a maximal list of verts such that no vert in the list has any neighbor in the list.
    public static int[] findMaximalIndependentVertexSet(Mesh mesh, java.util.Random rng)
    {
        int verboseLevel = 0;
        int nVerts = mesh.verts.size();
        int nEdges = mesh.edges.size();

        int answer[] = new int[nVerts]; // will shrink at end
        int answerSize = 0;

        Mesh.Edge vertToAnyEdgeOut[] = getVertToFirstEdgeOut(mesh);

        int vertIndexToCandidateIndex[] = VecMath.identityperm(nVerts);
        int candidates[] = VecMath.identityperm(nVerts);
        int nCandidates = nVerts;

        // Make verts incident on infinite edges ineligible
        FORI (iEdge, nEdges)
        {
            Mesh.Edge edge = mesh.getEdge(iEdge);
            if (edge.initialVertex() != null && edge.finalVertex() == null)
            {
                int iVert = edge.initialVertex().myIndex();
                int iCandidate = vertIndexToCandidateIndex[iVert];
                if (iCandidate != -1)
                {
                    assert_eq(candidates[iCandidate], iVert);
                    // dup code alert: same as below
                    vertIndexToCandidateIndex[candidates[nCandidates-1]] = iCandidate;
                    vertIndexToCandidateIndex[iVert] = -1;
                    candidates[iCandidate] = candidates[nCandidates-1];
                    candidates[nCandidates-1] = -1;
                    --nCandidates;
                }
            }
        }

        while (nCandidates > 0)
        {
            if (verboseLevel >= 2) OUT("      top of loop");
            if (verboseLevel >= 2) OUT("              answer = "+Arrays.toStringCompact(Arrays.subarray(answer, 0, answerSize)));
            if (verboseLevel >= 2) OUT("              candidates = "+Arrays.toStringCompact(Arrays.subarray(candidates, 0, nCandidates)));
            if (verboseLevel >= 2) OUT("              vertIndexToCandidateIndex = "+Arrays.toStringCompact(vertIndexToCandidateIndex));
            int iCandidate = rng.nextInt(nCandidates);
            if (verboseLevel >= 2) OUT("          iCandidate = "+iCandidate);
            int iVert = candidates[iCandidate];
            if (verboseLevel >= 2) OUT("          iVert = "+iVert);
            assert_eq(vertIndexToCandidateIndex[iVert], iCandidate);

            // Add it to the set
            answer[answerSize++] = iVert;

            // Remove it and all its neighbors from candidates list

            // dup code alert: same as below
            vertIndexToCandidateIndex[candidates[nCandidates-1]] = iCandidate;
            vertIndexToCandidateIndex[iVert] = -1;
            candidates[iCandidate] = candidates[nCandidates-1];
            candidates[nCandidates-1] = -1;
            --nCandidates;

            if (verboseLevel >= 2) OUT("          after transferring to answer:");
            if (verboseLevel >= 2) OUT("              answer = "+Arrays.toStringCompact(Arrays.subarray(answer, 0, answerSize)));
            if (verboseLevel >= 2) OUT("              candidates = "+Arrays.toStringCompact(Arrays.subarray(candidates, 0, nCandidates)));
            if (verboseLevel >= 2) OUT("              vertIndexToCandidateIndex = "+Arrays.toStringCompact(vertIndexToCandidateIndex));

            if (false)
            {
                // sanity check
                FORI (i, nCandidates)
                    assert_eq(vertIndexToCandidateIndex[candidates[i]], i);
                FORI (i, nVerts)
                    if (vertIndexToCandidateIndex[i] != -1)
                    {
                        assert_lt(vertIndexToCandidateIndex[i], nCandidates);
                        assert_eq(candidates[vertIndexToCandidateIndex[i]], i);
                    }
                for (int i = nCandidates; i < candidates.length; ++i)
                    assert_eq(candidates[i], -1);
            }

            Mesh.Edge edge0 = vertToAnyEdgeOut[iVert];
            assert_ne(edge0, null);
            Mesh.Edge edge = edge0;
            do {
                Mesh.Vertex finalVertex = edge.finalVertex();
                if (finalVertex != null)
                {
                    iVert = finalVertex.myIndex();
                    iCandidate = vertIndexToCandidateIndex[iVert];
                    if (iCandidate != -1)
                    {
                        if (verboseLevel >= 2) OUT("              neighbor v"+iVert+" is now ineligible");
                        assert_eq(candidates[iCandidate], iVert);

                        // dup code alert: same as above
                        vertIndexToCandidateIndex[candidates[nCandidates-1]] = iCandidate;
                        vertIndexToCandidateIndex[iVert] = -1;
                        candidates[iCandidate] = candidates[nCandidates-1];
                        candidates[nCandidates-1] = -1;
                        --nCandidates;
                    }
                    else
                    {
                        if (verboseLevel >= 2) OUT("              (neighbor v"+iVert+" already ineligible)");
                    }
                }
            } while ((edge = edge.opposite().next()) != edge0);
        }
        answer = (int[])Arrays.subarray(answer, 0, answerSize);
        return answer;
    } // findMaximalIndependentVertexSet

    // Observation: sometimes truncating a dual vertex
    // makes unfolding more difficult, in the sense that
    // something that was previously an unfolding is no longer.
    // See if we can exploit this.
    public static void maybeMakeMoreDifficultByTruncatingDual(Mesh mesh,
                                                              Mesh dualMesh,
                                                              double truncationFrac,
                                                              java.util.Random rng)
    {
        OUT("    in maybeMakeMoreDifficultByTruncatingDual");
        int nEdges = mesh.edges.size();
        assert_eq(nEdges, dualMesh.edges.size());
        int nDualVerts = dualMesh.verts.size();

        // First find a maximal independent set of dual vertices (i.e. no pair of neighbors), greedily...
        int maximalIndependentDualVertSet[] = findMaximalIndependentVertexSet(dualMesh, rng);
        OUT("      maximalIndependentDualVertSet = "+Arrays.toStringCompact(maximalIndependentDualVertSet));
        OUT("      maximalIndependentDualVertSet.length = "+maximalIndependentDualVertSet.length);
        Mesh.Edge dualVertToFirstDualEdgeOut[] = getVertToFirstEdgeOut(dualMesh);

        double scratch[] = new double[3];
        FORI (i, maximalIndependentDualVertSet.length)
        {
            int iDualVert = maximalIndependentDualVertSet[i];
            Mesh.Edge dualEdge0 = dualVertToFirstDualEdgeOut[iDualVert];
            // dual mesh is trivalent, so...
            assert(dualEdge0.opposite().next()
                            .opposite().next()
                            .opposite().next() == dualEdge0);
            Mesh.Vertex v0 = dualMesh.getVert(iDualVert);
            Mesh.Vertex v1 = dualEdge0.finalVertex();
            Mesh.Vertex v2 = dualEdge0.opposite().next().finalVertex();
            Mesh.Vertex v3 = dualEdge0.opposite().next().opposite().next().finalVertex();

            double x0 = v0.x(), y0 = v0.y(), h0 = v0.h();
            double x1 = v1.x(), y1 = v1.y(), h1 = v1.h();
            double x2 = v2.x(), y2 = v2.y(), h2 = v2.h();
            double x3 = v3.x(), y3 = v3.y(), h3 = v3.h();

            // convert from height-above-paraboloid to euclidean ("actual")
            h0 -= .5 * (SQR(x0) + SQR(y0));
            h1 -= .5 * (SQR(x1) + SQR(y1));
            h2 -= .5 * (SQR(x2) + SQR(y2));
            h3 -= .5 * (SQR(x3) + SQR(y3));

            // lerp in euclidean space
            x1 = LERP(x0, x1, truncationFrac); y1 = LERP(y0, y1, truncationFrac); h1 = LERP(h0, h1, truncationFrac);
            x2 = LERP(x0, x2, truncationFrac); y2 = LERP(y0, y2, truncationFrac); h2 = LERP(h0, h2, truncationFrac);
            x3 = LERP(x0, x3, truncationFrac); y3 = LERP(y0, y3, truncationFrac); h3 = LERP(h0, h3, truncationFrac);

            // convert from euclidean back to height-above-paraboloid
            h0 += .5 * (SQR(x0) + SQR(y0));
            h1 += .5 * (SQR(x1) + SQR(y1));
            h2 += .5 * (SQR(x2) + SQR(y2));
            h3 += .5 * (SQR(x3) + SQR(y3));

            GeomUtils.SolveForDualPoint(x1, y1, h1,
                                        x2, y2, h2,
                                        x3, y3, h3,
                                        scratch,
                                        false,false,0.);
            double x = scratch[0];
            double y = scratch[1];
            double h = scratch[2];

            mesh.addIsolatedVertex(x, y, h);
            mesh.kisIsolatedVertex(mesh.getVert(mesh.verts.size()-1),
                                   mesh.getEdge(dualEdge0.opposite().myIndex())); // I had 50% chance of getting this right
        }
        OUT("    out maybeMakeMoreDifficultByTruncatingDual");
    } // maybeMakeMoreDifficultByTruncating

}  // class MeshUtils
